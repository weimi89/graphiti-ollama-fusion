#!/usr/bin/env python3
"""
èª¿è©¦ Ollama å¯¦é«”æå–éŸ¿æ‡‰
"""
import asyncio
import json
from ollama_graphiti_client import OptimizedOllamaClient
from graphiti_core.llm_client.config import LLMConfig

async def test_ollama_response():
    """æ¸¬è©¦ Ollama çš„å¯¦éš›éŸ¿æ‡‰æ ¼å¼"""

    config = LLMConfig(
        api_key="not-needed",
        model="qwen2.5:14b",
        base_url="http://localhost:11434",
        temperature=0.1
    )

    client = OptimizedOllamaClient(config)

    # æ¸¬è©¦æ¶ˆæ¯
    messages = [{
        "role": "user",
        "content": "æå–é€™æ®µæ–‡å­—çš„å¯¦é«”: é€™æ˜¯ä¸€å€‹å¿«é€Ÿæ¸¬è©¦è¨˜æ†¶"
    }]

    print("ğŸ” æ¸¬è©¦ Ollama è«‹æ±‚å’ŒéŸ¿æ‡‰...")
    print(f"è¼¸å…¥: {messages[0]['content']}")

    try:
        # ç›´æ¥èª¿ç”¨åº•å±¤è«‹æ±‚æ–¹æ³•
        response_text = await client._make_request(messages, json_mode=True)
        print(f"\nğŸ“¤ åŸå§‹éŸ¿æ‡‰: {repr(response_text)}")

        # æ¸¬è©¦ JSON è§£æ
        json_data = client._extract_json_from_response(response_text)
        print(f"\nğŸ“‹ è§£æå¾Œçš„ JSON: {json_data}")

        # æª¢æŸ¥çµæ§‹
        if 'extracted_entities' in json_data:
            print(f"\nâœ… æ‰¾åˆ° extracted_entities å­—æ®µ")
            entities = json_data['extracted_entities']
            print(f"å¯¦é«”é¡å‹: {type(entities)}")
            print(f"å¯¦é«”å…§å®¹: {entities}")

            if isinstance(entities, list):
                print(f"å¯¦é«”æ•¸é‡: {len(entities)}")
                for i, entity in enumerate(entities):
                    print(f"  å¯¦é«” {i}: {entity} (é¡å‹: {type(entity)})")
        else:
            print("âŒ æ²’æœ‰æ‰¾åˆ° extracted_entities å­—æ®µ")
            print(f"å¯ç”¨å­—æ®µ: {list(json_data.keys())}")

    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_ollama_response())