#!/usr/bin/env python3
"""
Ollama Embedder for Graphiti
æ”¯æ´ä½¿ç”¨ Ollama çš„æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ nomic-embed-textï¼‰
"""

import asyncio
import aiohttp
import json
from typing import List, Optional
import numpy as np
from graphiti_core.embedder.client import EmbedderClient


class OllamaEmbedder(EmbedderClient):
    """
    è‡ªå®šç¾© Ollama åµŒå…¥å™¨ï¼Œå¯¦ä½œ Graphiti çš„ EmbedderClient ä»‹é¢
    """

    def __init__(
        self,
        model: str = "nomic-embed-text:v1.5",
        base_url: str = "http://localhost:11434",
        dimensions: int = 768  # nomic-embed-text çš„é è¨­ç¶­åº¦
    ):
        """
        åˆå§‹åŒ– Ollama åµŒå…¥å™¨

        Args:
            model: Ollama åµŒå…¥æ¨¡å‹åç¨±
            base_url: Ollama API ç«¯é»
            dimensions: åµŒå…¥å‘é‡ç¶­åº¦
        """
        self.model = model
        self.base_url = base_url.rstrip('/')
        self.dimensions = dimensions
        self.embed_url = f"{self.base_url}/api/embed"

    async def create(self, input_data: List[str]) -> List[List[float]]:
        """
        å‰µå»ºåµŒå…¥å‘é‡

        Args:
            input_data: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not input_data:
            return []

        embeddings = []

        # Ollama çš„åµŒå…¥ API ä¸€æ¬¡åªèƒ½è™•ç†ä¸€å€‹æ–‡æœ¬
        # æ‰€ä»¥æˆ‘å€‘éœ€è¦é€å€‹è™•ç†
        async with aiohttp.ClientSession() as session:
            for text in input_data:
                try:
                    payload = {
                        "model": self.model,
                        "input": text
                    }

                    async with session.post(
                        self.embed_url,
                        json=payload,
                        headers={"Content-Type": "application/json"}
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            embeddings_list = result.get("embeddings", [])
                            embedding = embeddings_list[0] if embeddings_list else []

                            # ç¢ºä¿åµŒå…¥å‘é‡ç¶­åº¦æ­£ç¢º
                            if len(embedding) != self.dimensions:
                                # èª¿æ•´ç¶­åº¦ï¼ˆæˆªæ–·æˆ–å¡«å……ï¼‰
                                if len(embedding) > self.dimensions:
                                    embedding = embedding[:self.dimensions]
                                else:
                                    embedding.extend([0.0] * (self.dimensions - len(embedding)))

                            # æª¢æŸ¥æ˜¯å¦ç‚ºé›¶å‘é‡ä¸¦æ­¸ä¸€åŒ–ï¼ˆç¢ºä¿ cosine similarity æ­£ç¢ºï¼‰
                            vector_norm = sum(x*x for x in embedding) ** 0.5
                            if vector_norm == 0.0:
                                print(f"âš ï¸ æª¢æ¸¬åˆ°é›¶å‘é‡ï¼Œä½¿ç”¨éš¨æ©Ÿå°å‘é‡æ›¿ä»£: '{text[:50]}...'")
                                # ç”Ÿæˆä¸€å€‹å°çš„éš¨æ©Ÿå‘é‡ä¾†é¿å… cosine similarity éŒ¯èª¤
                                import random
                                embedding = [random.uniform(-0.01, 0.01) for _ in range(self.dimensions)]
                                # é‡æ–°è¨ˆç®—å‘é‡ç¯„æ•¸
                                vector_norm = sum(x*x for x in embedding) ** 0.5

                            # ç¢ºä¿å‘é‡æ­¸ä¸€åŒ–ï¼ˆNeo4j cosine similarity è¦æ±‚å–®ä½å‘é‡ï¼‰
                            if vector_norm > 0:
                                embedding = [x / vector_norm for x in embedding]

                            embeddings.append(embedding)
                        else:
                            error_text = await response.text()
                            print(f"âŒ Ollama åµŒå…¥éŒ¯èª¤ (ç‹€æ…‹ {response.status}): {error_text}")
                            # è¿”å›æ­¸ä¸€åŒ–çš„éš¨æ©Ÿå‘é‡é¿å… cosine similarity éŒ¯èª¤
                            import random
                            fallback_embedding = [random.uniform(-0.01, 0.01) for _ in range(self.dimensions)]
                            # æ­¸ä¸€åŒ–éš¨æ©Ÿå‘é‡
                            vector_norm = sum(x*x for x in fallback_embedding) ** 0.5
                            if vector_norm > 0:
                                fallback_embedding = [x / vector_norm for x in fallback_embedding]
                            embeddings.append(fallback_embedding)

                except Exception as e:
                    print(f"âŒ åµŒå…¥è«‹æ±‚å¤±æ•—: {str(e)}")
                    # è¿”å›æ­¸ä¸€åŒ–çš„éš¨æ©Ÿå‘é‡é¿å… cosine similarity éŒ¯èª¤
                    import random
                    fallback_embedding = [random.uniform(-0.01, 0.01) for _ in range(self.dimensions)]
                    # æ­¸ä¸€åŒ–éš¨æ©Ÿå‘é‡
                    vector_norm = sum(x*x for x in fallback_embedding) ** 0.5
                    if vector_norm > 0:
                        fallback_embedding = [x / vector_norm for x in fallback_embedding]
                    embeddings.append(fallback_embedding)

        return embeddings

    async def create_bulk(self, input_data: List[str], batch_size: int = 10) -> List[List[float]]:
        """
        æ‰¹é‡å‰µå»ºåµŒå…¥å‘é‡ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰

        Args:
            input_data: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆä¸¦ç™¼è«‹æ±‚æ•¸ï¼‰

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not input_data:
            return []

        embeddings = []

        async def embed_single(session: aiohttp.ClientSession, text: str) -> List[float]:
            """åµŒå…¥å–®å€‹æ–‡æœ¬"""
            try:
                payload = {
                    "model": self.model,
                    "input": text
                }

                async with session.post(
                    self.embed_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        embeddings_list = result.get("embeddings", [])
                        embedding = embeddings_list[0] if embeddings_list else []

                        # èª¿æ•´ç¶­åº¦
                        if len(embedding) != self.dimensions:
                            if len(embedding) > self.dimensions:
                                embedding = embedding[:self.dimensions]
                            else:
                                embedding.extend([0.0] * (self.dimensions - len(embedding)))

                        # ç¢ºä¿å‘é‡æ­¸ä¸€åŒ–
                        vector_norm = sum(x*x for x in embedding) ** 0.5
                        if vector_norm == 0.0:
                            # ç”Ÿæˆéš¨æ©Ÿå‘é‡
                            import random
                            embedding = [random.uniform(-0.01, 0.01) for _ in range(self.dimensions)]
                            vector_norm = sum(x*x for x in embedding) ** 0.5

                        # æ­¸ä¸€åŒ–å‘é‡
                        if vector_norm > 0:
                            embedding = [x / vector_norm for x in embedding]

                        return embedding
                    else:
                        print(f"âŒ åµŒå…¥éŒ¯èª¤: {response.status}")
                        # è¿”å›æ­¸ä¸€åŒ–çš„éš¨æ©Ÿå‘é‡
                        import random
                        fallback_embedding = [random.uniform(-0.01, 0.01) for _ in range(self.dimensions)]
                        vector_norm = sum(x*x for x in fallback_embedding) ** 0.5
                        if vector_norm > 0:
                            fallback_embedding = [x / vector_norm for x in fallback_embedding]
                        return fallback_embedding

            except Exception as e:
                print(f"âŒ åµŒå…¥è«‹æ±‚å¤±æ•—: {str(e)}")
                # è¿”å›æ­¸ä¸€åŒ–çš„éš¨æ©Ÿå‘é‡
                import random
                fallback_embedding = [random.uniform(-0.01, 0.01) for _ in range(self.dimensions)]
                vector_norm = sum(x*x for x in fallback_embedding) ** 0.5
                if vector_norm > 0:
                    fallback_embedding = [x / vector_norm for x in fallback_embedding]
                return fallback_embedding

        async with aiohttp.ClientSession() as session:
            # åˆ†æ‰¹è™•ç†ä»¥é¿å…éè¼‰
            for i in range(0, len(input_data), batch_size):
                batch = input_data[i:i + batch_size]
                batch_results = await asyncio.gather(
                    *[embed_single(session, text) for text in batch]
                )
                embeddings.extend(batch_results)

        return embeddings

    async def create_batch(self, input_data: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡å‰µå»ºåµŒå…¥ï¼ˆGraphiti éœ€è¦çš„æ–¹æ³•ï¼‰

        Args:
            input_data: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        return await self.create(input_data)

    def get_dimensions(self) -> int:
        """
        ç²å–åµŒå…¥å‘é‡ç¶­åº¦

        Returns:
            åµŒå…¥å‘é‡ç¶­åº¦
        """
        return self.dimensions

    async def test_connection(self) -> bool:
        """
        æ¸¬è©¦ Ollama é€£æ¥

        Returns:
            é€£æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            async with aiohttp.ClientSession() as session:
                # æ¸¬è©¦ API ç«¯é»
                async with session.get(f"{self.base_url}/api/tags") as response:
                    if response.status == 200:
                        result = await response.json()
                        models = result.get("models", [])

                        # æª¢æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å¯ç”¨
                        model_names = [m.get("name", "") for m in models]
                        if self.model in model_names:
                            print(f"âœ… Ollama åµŒå…¥å™¨é€£æ¥æˆåŠŸï¼Œæ¨¡å‹ {self.model} å¯ç”¨")
                            return True
                        else:
                            print(f"âš ï¸ Ollama é€£æ¥æˆåŠŸï¼Œä½†æ¨¡å‹ {self.model} æœªæ‰¾åˆ°")
                            print(f"   å¯ç”¨æ¨¡å‹: {', '.join(model_names)}")
                            return False
                    else:
                        print(f"âŒ Ollama é€£æ¥å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status}")
                        return False

        except Exception as e:
            print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama: {str(e)}")
            return False

    async def get_model_info(self) -> dict:
        """
        ç²å–æ¨¡å‹ä¿¡æ¯

        Returns:
            æ¨¡å‹è©³ç´°ä¿¡æ¯
        """
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/show",
                    json={"name": self.model}
                ) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        return {"error": f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯: {response.status}"}

        except Exception as e:
            return {"error": str(e)}


# æ¸¬è©¦å‡½æ•¸
async def test_ollama_embedder():
    """æ¸¬è©¦ Ollama åµŒå…¥å™¨"""

    print("ğŸ§ª æ¸¬è©¦ Ollama åµŒå…¥å™¨")
    print("=" * 50)

    # å‰µå»ºåµŒå…¥å™¨
    embedder = OllamaEmbedder(
        model="nomic-embed-text:v1.5",
        base_url="http://localhost:11434"
    )

    # æ¸¬è©¦é€£æ¥
    print("\n1ï¸âƒ£ æ¸¬è©¦é€£æ¥...")
    connected = await embedder.test_connection()
    if not connected:
        print("   âŒ é€£æ¥å¤±æ•—ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ")
        return False

    # æ¸¬è©¦å–®å€‹åµŒå…¥
    print("\n2ï¸âƒ£ æ¸¬è©¦å–®å€‹æ–‡æœ¬åµŒå…¥...")
    test_texts = ["TypeScript æ˜¯ JavaScript çš„è¶…é›†"]
    embeddings = await embedder.create(test_texts)

    if embeddings and len(embeddings[0]) > 0:
        print(f"   âœ… æˆåŠŸï¼åµŒå…¥ç¶­åº¦: {len(embeddings[0])}")
        print(f"   å‰5å€‹å€¼: {embeddings[0][:5]}")
    else:
        print("   âŒ åµŒå…¥å¤±æ•—")
        return False

    # æ¸¬è©¦æ‰¹é‡åµŒå…¥
    print("\n3ï¸âƒ£ æ¸¬è©¦æ‰¹é‡åµŒå…¥...")
    batch_texts = [
        "React 18 å¼•å…¥äº† Concurrent Features",
        "API éŒ¯èª¤è™•ç†æœ€ä½³å¯¦è¸",
        "ç”¨æˆ¶åå¥½ä½¿ç”¨ TypeScript"
    ]
    batch_embeddings = await embedder.create_bulk(batch_texts, batch_size=2)

    if len(batch_embeddings) == len(batch_texts):
        print(f"   âœ… æˆåŠŸåµŒå…¥ {len(batch_embeddings)} å€‹æ–‡æœ¬")
        for i, text in enumerate(batch_texts):
            print(f"   - '{text[:30]}...' -> ç¶­åº¦ {len(batch_embeddings[i])}")
    else:
        print("   âŒ æ‰¹é‡åµŒå…¥å¤±æ•—")
        return False

    # ç²å–æ¨¡å‹ä¿¡æ¯
    print("\n4ï¸âƒ£ ç²å–æ¨¡å‹ä¿¡æ¯...")
    model_info = await embedder.get_model_info()
    if "error" not in model_info:
        print(f"   âœ… æ¨¡å‹: {embedder.model}")
        if "modelfile" in model_info:
            lines = model_info["modelfile"].split('\n')[:3]
            for line in lines:
                if line:
                    print(f"   - {line}")

    print("\nâœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼")
    return True


if __name__ == "__main__":
    # åŸ·è¡Œæ¸¬è©¦
    import asyncio
    success = asyncio.run(test_ollama_embedder())
    exit(0 if success else 1)