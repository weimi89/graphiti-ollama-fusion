# Graphiti MCP Server

ğŸ‡¹ğŸ‡¼ **æœ¬åœ°åŒ–çŸ¥è­˜åœ–è­œè¨˜æ†¶æœå‹™** - æ•´åˆ Ollama æœ¬åœ° LLM èˆ‡ Graphiti çš„ä¼æ¥­ç´š MCP æœå‹™å™¨

## ğŸŒŸ ç‰¹è‰²åŠŸèƒ½

- ğŸ§  **æ™ºèƒ½è¨˜æ†¶ç®¡ç†** - ä½¿ç”¨çŸ¥è­˜åœ–è­œå„²å­˜å’Œæª¢ç´¢è¤‡é›œçš„è¨˜æ†¶é—œä¿‚
- ğŸ” **èªæ„æœå°‹** - åŸºæ–¼å‘é‡åµŒå…¥çš„æ™ºèƒ½æœå°‹ï¼Œç†è§£èªæ„è€Œéåƒ…æ–‡å­—åŒ¹é…
- ğŸ  **å®Œå…¨æœ¬åœ°åŒ–** - ç„¡éœ€å¤–éƒ¨ APIï¼Œä½¿ç”¨ Ollama æœ¬åœ° LLM å’ŒåµŒå…¥æ¨¡å‹
- ğŸ‡¹ğŸ‡¼ **ç¹é«”ä¸­æ–‡** - å®Œæ•´çš„ä¸­æ–‡ç•Œé¢å’Œå›æ‡‰ï¼Œå°ˆç‚ºå°ç£ç”¨æˆ¶è¨­è¨ˆ
- ğŸ—ï¸ **ä¼æ¥­ç´šæ¶æ§‹** - çµæ§‹åŒ–é…ç½®ã€ç•°å¸¸è™•ç†ã€æ—¥èªŒç³»çµ±å’Œç›£æ§

## ğŸ—ï¸ å°ˆæ¡ˆçµæ§‹

```
graphiti/
â”œâ”€â”€ src/                          # æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ config.py                 # é…ç½®ç®¡ç†ç³»çµ±
â”‚   â”œâ”€â”€ exceptions.py             # çµæ§‹åŒ–ç•°å¸¸è™•ç†
â”‚   â”œâ”€â”€ logging_setup.py          # æ—¥èªŒè¨˜éŒ„ç³»çµ±
â”‚   â”œâ”€â”€ ollama_embedder.py        # Ollama åµŒå…¥å™¨
â”‚   â””â”€â”€ ollama_graphiti_client.py # Ollama LLM å®¢æˆ¶ç«¯
â”œâ”€â”€ tools/                        # å¯¦ç”¨å·¥å…·
â”œâ”€â”€ docs/                         # æ–‡æª”
â”‚   â””â”€â”€ ä½¿ç”¨å·¥å…·çš„æŒ‡ä»¤.md          # å·¥å…·ä½¿ç”¨æŒ‡ä»¤å’Œæœ€ä½³å¯¦è¸
â”œâ”€â”€ logs/                         # æ—¥èªŒæª”æ¡ˆ
â””â”€â”€ graphiti_mcp_server.py        # ä¸»æœå‹™å™¨
```

## ğŸš€ å¿«é€Ÿå•Ÿå‹•

> **ğŸ“– é‡è¦æé†’ï¼š** è¨­å®šå®Œæˆå¾Œï¼Œè«‹å‹™å¿…é–±è®€ [ä½¿ç”¨å·¥å…·çš„æŒ‡ä»¤](docs/ä½¿ç”¨å·¥å…·çš„æŒ‡ä»¤.md) ä»¥äº†è§£å¦‚ä½•æ­£ç¢ºä½¿ç”¨ Graphiti MCP å·¥å…·ï¼

### 1. ç³»çµ±éœ€æ±‚

- **Python**: 3.11+
- **Neo4j**: 4.0+ (bolt://localhost:7687)
- **Ollama**: æœ¬åœ°é‹è¡Œ (http://localhost:11434)
- **å¿…éœ€æ¨¡å‹**:
  - `qwen2.5:7b` (LLM)
  - `nomic-embed-text:v1.5` (åµŒå…¥)

### ğŸ’» ç¡¬é«”æ•ˆèƒ½å»ºè­°

é¸æ“‡åˆé©çš„æ¨¡å‹æ­é…æ‚¨çš„é›»è…¦æ•ˆèƒ½ï¼Œç²å¾—æœ€ä½³é«”é©—ï¼š

#### M1/M2 Mac (8-16GB RAM)
```bash
# ğŸ† æœ€ä½³æ¨è–¦çµ„åˆ - é€Ÿåº¦èˆ‡å“è³ªå®Œç¾å¹³è¡¡
ollama pull qwen2.5:3b        # LLM (0.72ç§’å›æ‡‰ï¼Œå“è³ªå¾ˆå¥½)
ollama pull nomic-embed-text:v1.5

# âš¡ æ¥µé€Ÿçµ„åˆ - å„ªå…ˆå›æ‡‰é€Ÿåº¦
ollama pull qwen2.5:0.5b      # LLM (0.68ç§’å›æ‡‰ï¼ŒåŸºæœ¬å“è³ª)
ollama pull nomic-embed-text:v1.5

# ğŸ’ é«˜å“è³ªçµ„åˆ - å°ˆæ¥­ç”¨é€”
ollama pull qwen2.5:7b        # LLM (1.50ç§’å›æ‡‰ï¼Œå„ªç§€å“è³ª)
ollama pull nomic-embed-text:v1.5
```

#### Intel/AMD æ¡Œæ©Ÿ (16GB+ RAM)
```bash
# å¹³è¡¡çµ„åˆ
ollama pull qwen2.5:3b        # æˆ– llama3.2:3b
ollama pull nomic-embed-text:v1.5

# é«˜æ€§èƒ½çµ„åˆ (32GB+ RAM)
ollama pull qwen2.5:7b        # åŸå»ºè­°æ¨¡å‹
ollama pull nomic-embed-text:v1.5
```

#### æ•ˆèƒ½æ¯”è¼ƒè¡¨
| æ¨¡å‹ | å¤§å° | GPUè¨˜æ†¶é«” | å›æ‡‰æ™‚é–“* | å›æ‡‰å“è³ª | æ¨è–¦æŒ‡æ•¸ |
|------|------|----------|----------|----------|----------|
| **qwen2.5:0.5b** | 397 MB | 1.3 GB | 0.68ç§’ | åŸºæœ¬ | â­â­â­â­â­ |
| **qwen2.5:1.5b** | 986 MB | 1.9 GB | 0.71ç§’ | è‰¯å¥½ | â­â­â­â­â­ |
| **qwen2.5:3b** | 1.9 GB | 2.0 GB | 0.72ç§’ | å¾ˆå¥½ | â­â­â­â­â­ |
| **qwen2.5:7b** | 4.7 GB | 4.0+ GB | 1.50ç§’ | å„ªç§€ | â­â­â­â­ |
| llama3.2:1b | 1.3 GB | 1.5 GB | 1.03ç§’ | ä¸­ç­‰ | â­â­â­ |
| gemma3:1b | 815 MB | 1.9 GB | 0.87ç§’ | è‰¯å¥½ | â­â­â­â­ |
| deepseek-r1:1.5b | 1.1 GB | 2.0 GB | 2.31ç§’â€  | åˆ†æå‹ | â­â­â­ |

> **è¨»ï¼š** *å¯¦æ¸¬æ–¼ M2 MacBook Proï¼Œâ€  R1æ¨¡å‹åŒ…å«æ€è€ƒéç¨‹è¼ƒæ…¢

> **âš ï¸ é‡è¦æé†’**ï¼šä»¥ä¸Šæ•¸æ“šä¾å€‹äººé›»è…¦çš„æ¸¬è©¦è€Œæœ‰æ‰€ä¸åŒï¼Œå»ºè­°å…ˆåšå¥½æ¸¬è©¦å†é¸æ“‡æ¨¡çµ„ã€‚

#### ğŸ“Š æ€§èƒ½æ¸¬è©¦æŒ‡å—
åœ¨é¸æ“‡æ¨¡å‹å‰ï¼Œå»ºè­°å…ˆæ¸¬è©¦å„æ¨¡å‹åœ¨æ‚¨ç¡¬é«”ä¸Šçš„å¯¦éš›è¡¨ç¾ï¼š

```bash
# æ¸¬è©¦å„æ¨¡å‹å›æ‡‰æ™‚é–“
time ollama run qwen2.5:0.5b "ä½ å¥½ï¼Œè«‹ç°¡çŸ­å›æ‡‰"
time ollama run qwen2.5:1.5b "ä½ å¥½ï¼Œè«‹ç°¡çŸ­å›æ‡‰"
time ollama run qwen2.5:3b "ä½ å¥½ï¼Œè«‹ç°¡çŸ­å›æ‡‰"

# æª¢æŸ¥ç³»çµ±è³‡æºä½¿ç”¨
top -l 1 -n 0 | grep -E "CPU|Memory"
ollama ps  # æŸ¥çœ‹æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨é‡
```

#### åˆ‡æ›æ¨¡å‹è¨­å®š
åœ¨é…ç½®æª”æ¡ˆä¸­ä¿®æ”¹æ¨¡å‹åç¨±ï¼š
```json
{
  "ollama": {
    "model": "qwen2.5:1.5b",  // æ”¹ç‚ºé©åˆçš„æ¨¡å‹
    "base_url": "http://localhost:11434"
  }
}
```

### 2. å®‰è£ Python å’Œ uv

#### Python 3.11+ å®‰è£

**macOS:**
```bash
# ä½¿ç”¨ Homebrew å®‰è£ (æ¨è–¦)
brew install python@3.11

# æˆ–ä½¿ç”¨ pyenv ç®¡ç†å¤šç‰ˆæœ¬
brew install pyenv
pyenv install 3.11.0
pyenv global 3.11.0

# é©—è­‰å®‰è£
python3 --version  # æ‡‰é¡¯ç¤º 3.11.x
```

**Windows:**
```powershell
# ä½¿ç”¨ Chocolatey å®‰è£
choco install python --version=3.11.0

# æˆ–ä½¿ç”¨ Scoop å®‰è£
scoop install python

# æˆ–å¾å®˜ç¶²ä¸‹è¼‰å®‰è£
# https://www.python.org/downloads/windows/

# é©—è­‰å®‰è£
python --version  # æ‡‰é¡¯ç¤º 3.11.x
```

**Linux (Ubuntu/Debian):**
```bash
# æ›´æ–°å¥—ä»¶åˆ—è¡¨
sudo apt update

# å®‰è£ Python 3.11
sudo apt install python3.11 python3.11-pip python3.11-venv

# è¨­å®šé è¨­ç‰ˆæœ¬
sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# é©—è­‰å®‰è£
python3 --version  # æ‡‰é¡¯ç¤º 3.11.x
```

#### uv å¥—ä»¶ç®¡ç†å™¨å®‰è£

**macOS/Linux:**
```bash
# ä½¿ç”¨å®˜æ–¹å®‰è£è…³æœ¬ (æ¨è–¦)
curl -LsSf https://astral.sh/uv/install.sh | sh

# æˆ–ä½¿ç”¨ Homebrew (macOS)
brew install uv

# æˆ–ä½¿ç”¨ pip å®‰è£
pip install uv

# é©—è­‰å®‰è£
uv --version
```

**Windows:**
```powershell
# ä½¿ç”¨ PowerShell å®‰è£è…³æœ¬
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# æˆ–ä½¿ç”¨ Scoop
scoop install uv

# æˆ–ä½¿ç”¨ pip å®‰è£
pip install uv

# é©—è­‰å®‰è£
uv --version
```

### 3. å®‰è£å°ˆæ¡ˆä¾è³´

```bash
# å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/weimi89/graphiti-ollama-fusion.git
cd graphiti-mcp-server

# ä½¿ç”¨ uv å®‰è£ä¾è³´
uv sync

# å¦‚æœæ²’æœ‰ uvï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å‚³çµ±æ–¹å¼
# python -m venv .venv
# source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate     # Windows
# pip install -r requirements.txt
```

#### å®‰è£å…¶ä»–å¿…éœ€æœå‹™

**Neo4j åœ–è³‡æ–™åº«:**
```bash
# macOS (ä½¿ç”¨ Homebrew)
brew install neo4j
brew services start neo4j

# Windows (ä½¿ç”¨ Chocolatey)
choco install neo4j-community

# Linux (Ubuntu/Debian)
wget -O - https://debian.neo4j.com/neotechnology.gpg.key | sudo apt-key add -
echo 'deb https://debian.neo4j.com stable 4.0' | sudo tee /etc/apt/sources.list.d/neo4j.list
sudo apt update && sudo apt install neo4j

# Docker æ–¹å¼ (è·¨å¹³å°)
docker run -d -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/your_password \
  neo4j:5

# é©—è­‰å®‰è£ - ç€è¦½å™¨é–‹å•Ÿ http://localhost:7474
```

**Ollama æœ¬åœ° LLM:**
```bash
# macOS
brew install ollama
ollama serve  # å•Ÿå‹•æœå‹™

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve  # å•Ÿå‹•æœå‹™

# Windows - å¾å®˜ç¶²ä¸‹è¼‰
# https://ollama.ai/download/windows

# ä¸‹è¼‰å¿…éœ€æ¨¡å‹
ollama pull qwen2.5:7b
ollama pull nomic-embed-text:v1.5

# é©—è­‰å®‰è£
ollama list  # æª¢æŸ¥å·²å®‰è£çš„æ¨¡å‹
```

### 4. é…ç½®ç’°å¢ƒ

```bash
# è¤‡è£½ç’°å¢ƒè®Šæ•¸ç¯„ä¾‹
cp .env.example .env

# ç·¨è¼¯é…ç½® (è¨­å®š Neo4j å¯†ç¢¼ç­‰)
nano .env
```

### 5. å•Ÿå‹•æœå‹™

```bash
# STDIO æ¨¡å¼ (Claude Desktop ä½¿ç”¨)
uv run python graphiti_mcp_server.py --transport stdio

# SSE æ¨¡å¼ (ç¶²é å®¢æˆ¶ç«¯ä½¿ç”¨)
uv run python graphiti_mcp_server.py --transport sse --port 8000

# ä½¿ç”¨è‡ªå®šç¾©é…ç½®
uv run python graphiti_mcp_server.py --config your_config.json --transport sse
```

## ğŸ”— MCP å®¢æˆ¶ç«¯è¨­å®š

### æ¨¡å¼é¸æ“‡

æœ¬æœå‹™å™¨æ”¯æ´å…©ç¨®é‹è¡Œæ¨¡å¼ï¼š

| æ¨¡å¼ | é©ç”¨å ´æ™¯ | å„ªé» | ç¼ºé» |
|------|----------|------|------|
| **STDIO** | Claude Desktop, MCP Inspector | ç›´æ¥æ•´åˆã€ç©©å®š | åƒ…é™æœ¬åœ°ä½¿ç”¨ |
| **SSE** | ç¶²é æ‡‰ç”¨ã€é ç«¯å®¢æˆ¶ç«¯ | ç¶²è·¯å­˜å–ã€éˆæ´»éƒ¨ç½² | éœ€è¦ç¶²è·¯é…ç½® |

---

### æ¨¡å¼ä¸€ï¼šSTDIO æ¨¡å¼ï¼ˆé©ç”¨æ–¼ Claude Desktop CLIï¼‰

> **âš ï¸ é‡è¦ï¼š** STDIO æ¨¡å¼åƒ…é©ç”¨æ–¼ **Claude Desktop CLI** ç‰ˆæœ¬ï¼Œä¸é©ç”¨æ–¼ IDE æ•´åˆã€‚

#### Claude Desktop CLI è¨­å®š

**é…ç½®æª”æ¡ˆä½ç½®ï¼š**
- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
- **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`

**å®Œæ•´é…ç½®ç¯„ä¾‹ï¼š**
```json
{
  "mcpServers": {
    "graphiti-memory": {
      "command": "uv",
      "args": [
        "run",
        "--directory",
        "/path/to/your/graphiti",
        "python",
        "graphiti_mcp_server.py",
        "--transport",
        "stdio"
      ],
      "env": {
        "NEO4J_URI": "bolt://localhost:7687",
        "NEO4J_USER": "neo4j",
        "NEO4J_PASSWORD": "your_password",
        "NEO4J_DATABASE": "graphiti-db",
        "OPENAI_API_KEY": "ollama",
        "OPENAI_BASE_URL": "http://localhost:11434/v1",
        "MODEL_NAME": "qwen2.5:7b",
        "EMBEDDER_MODEL_NAME": "nomic-embed-text:v1.5",
        "GROUP_ID": "claude_desktop",
        "SEMAPHORE_LIMIT": "3",
        "LOG_FILE": "logs/graphiti_mcp_server.log",
        "LOG_LEVEL": "INFO",
        "OLLAMA_MODEL": "qwen2.5:7b",
        "OLLAMA_TEMPERATURE": "0.1",
        "OLLAMA_EMBEDDING_MODEL": "nomic-embed-text:v1.5",
        "OLLAMA_EMBEDDING_DIMENSIONS": "768",
        "OLLAMA_BASE_URL": "http://localhost:11434",
        "SEARCH_LIMIT": "20",
        "ENABLE_DEDUPLICATION": "true",
        "PYDANTIC_VALIDATION_FIXES": "true",
        "COSINE_SIMILARITY_THRESHOLD": "0.8"
      }
    }
  }
}
```

**å•Ÿå‹•æ¸¬è©¦ï¼š**
```bash
# æ‰‹å‹•æ¸¬è©¦ STDIO æ¨¡å¼
cd /path/to/your/graphiti
uv run python graphiti_mcp_server.py --transport stdio

# ä½¿ç”¨ MCP Inspector æ¸¬è©¦
npx @modelcontextprotocol/inspector uv run python graphiti_mcp_server.py --transport stdio
```

---

### æ¨¡å¼äºŒï¼šSSE æ¨¡å¼ï¼ˆé©ç”¨æ–¼ç¶²é æ‡‰ç”¨ï¼‰

#### åŸºæœ¬ SSE æœå‹™å™¨è¨­å®š

**1. å•Ÿå‹• SSE æœå‹™å™¨**
```bash
# é è¨­ç«¯å£ 8000
uv run python graphiti_mcp_server.py --transport sse

# è‡ªå®šç¾©ç«¯å£å’Œä¸»æ©Ÿ
uv run python graphiti_mcp_server.py --transport sse --host 0.0.0.0 --port 8080

# ä½¿ç”¨é…ç½®æª”æ¡ˆ
uv run python graphiti_mcp_server.py --config your_config.json --transport sse
```

**2. æœå‹™å™¨ç‹€æ…‹æª¢æŸ¥**
```bash
# æª¢æŸ¥æœå‹™å™¨æ˜¯å¦é‹è¡Œ
curl -f http://localhost:8000/health || echo "æœå‹™å™¨æœªé‹è¡Œ"

# æŸ¥çœ‹å¯ç”¨å·¥å…·
curl http://localhost:8000/tools
```

#### Claude Desktop SSE æ¨¡å¼è¨­å®š

å¦‚æœä½ æƒ³åœ¨ Claude Desktop ä¸­ä½¿ç”¨ SSE æ¨¡å¼ï¼ˆé©ç”¨æ–¼é ç«¯æœå‹™å™¨æˆ–å®¹å™¨åŒ–éƒ¨ç½²ï¼‰ï¼Œå¯ä»¥é€™æ¨£é…ç½®ï¼š

**é…ç½®æª”æ¡ˆä½ç½®ï¼š**
- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
- **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`

**SSE æ¨¡å¼é…ç½®ç¯„ä¾‹ï¼š**
```json
{
  "mcpServers": {
    "graphiti-memory-sse": {
      "command": "curl",
      "args": [
        "-X", "POST",
        "-H", "Content-Type: application/json",
        "-d", "{\"method\":\"initialize\",\"params\":{}}",
        "http://localhost:8000/mcp"
      ],
      "env": {
        "MCP_SERVER_URL": "http://localhost:8000"
      }
    }
  }
}
```

**é ç«¯æœå‹™å™¨ SSE é…ç½®ï¼š**
```json
{
  "mcpServers": {
    "graphiti-memory-remote": {
      "transport": {
        "type": "sse",
        "url": "http://your-server-ip:8000/sse"
      },
      "env": {
        "MCP_API_KEY": "your_api_key_if_needed"
      }
    }
  }
}
```

**ä½¿ç”¨ Docker éƒ¨ç½²çš„ SSE é…ç½®ï¼š**
```json
{
  "mcpServers": {
    "graphiti-memory-docker": {
      "transport": {
        "type": "sse",
        "url": "http://localhost:8000/sse"
      },
      "env": {
        "DOCKER_CONTAINER": "graphiti-mcp"
      }
    }
  }
}
```

#### ç¶²é å®¢æˆ¶ç«¯æ•´åˆ

**JavaScript å®¢æˆ¶ç«¯ç¯„ä¾‹ï¼š**
```javascript
// SSE é€£æ¥
const eventSource = new EventSource('http://localhost:8000/sse');

eventSource.onmessage = function(event) {
    const data = JSON.parse(event.data);
    console.log('æ”¶åˆ°è¨Šæ¯:', data);
};

// èª¿ç”¨ MCP å·¥å…·
async function addMemory(name, content, groupId = 'web_client') {
    const response = await fetch('http://localhost:8000/call', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
        },
        body: JSON.stringify({
            method: 'tools/call',
            params: {
                name: 'add_memory_simple',
                arguments: {
                    name: name,
                    episode_body: content,
                    group_id: groupId
                }
            }
        })
    });

    return await response.json();
}

// æœå°‹è¨˜æ†¶
async function searchMemory(query, maxResults = 10) {
    const response = await fetch('http://localhost:8000/call', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
        },
        body: JSON.stringify({
            method: 'tools/call',
            params: {
                name: 'search_memory_nodes',
                arguments: {
                    query: query,
                    max_nodes: maxResults
                }
            }
        })
    });

    return await response.json();
}
```

#### Docker éƒ¨ç½²ï¼ˆSSE æ¨¡å¼ï¼‰

**Dockerfile ç¯„ä¾‹ï¼š**
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .

RUN pip install uv
RUN uv sync

EXPOSE 8000

CMD ["uv", "run", "python", "graphiti_mcp_server.py", "--transport", "sse", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml ç¯„ä¾‹ï¼š**
```yaml
version: '3.8'
services:
  graphiti-mcp:
    build: .
    ports:
      - "8000:8000"
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=your_password
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - neo4j
      - ollama

  neo4j:
    image: neo4j:5
    ports:
      - "7687:7687"
      - "7474:7474"
    environment:
      - NEO4J_AUTH=neo4j/your_password

  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    command: ["serve"]

volumes:
  ollama_data:
```

### å…¶ä»– MCP å®¢æˆ¶ç«¯è¨­å®š

#### 1. Inspector æ¨¡å¼ï¼ˆèª¿è©¦ç”¨ï¼‰
```bash
# ä½¿ç”¨ MCP Inspector æ¸¬è©¦
npx @modelcontextprotocol/inspector uv run python graphiti_mcp_server.py --transport stdio
```

#### 2. è‡ªå®šç¾© MCP å®¢æˆ¶ç«¯
```python
from mcp import ClientSession, StdioServerParameters
import asyncio

async def main():
    server_params = StdioServerParameters(
        command="uv",
        args=[
            "run", "python", "graphiti_mcp_server.py",
            "--transport", "stdio"
        ],
        env={
            "NEO4J_PASSWORD": "your_password"
        }
    )

    async with ClientSession(server_params) as session:
        # ä½¿ç”¨ MCP å·¥å…·
        result = await session.call_tool(
            "add_memory_simple",
            {
                "name": "æ¸¬è©¦è¨˜æ†¶",
                "episode_body": "é€™æ˜¯ä¸€å€‹æ¸¬è©¦è¨˜æ†¶ç‰‡æ®µ",
                "group_id": "test_group"
            }
        )
        print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### é…ç½®æª¢æŸ¥

å•Ÿå‹• Claude Desktop å¾Œï¼Œä½ æ‡‰è©²èƒ½åœ¨å·¥å…·åˆ—è¡¨ä¸­çœ‹åˆ°ä»¥ä¸‹ MCP å·¥å…·ï¼š

- `add_memory_simple` - æ·»åŠ è¨˜æ†¶ç‰‡æ®µ
- `search_memory_nodes` - æœå°‹è¨˜æ†¶ç¯€é»
- `search_memory_facts` - æœå°‹è¨˜æ†¶äº‹å¯¦
- `get_episodes` - ç²å–è¨˜æ†¶ç‰‡æ®µ
- `test_connection` - æ¸¬è©¦é€£æ¥
- `clear_graph` - æ¸…é™¤åœ–è³‡æ–™åº«

### æ•…éšœæ’é™¤

å¦‚æœ MCP é€£æ¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ï¼š

1. **è·¯å¾‘è¨­å®š**
   ```bash
   # ç¢ºèªå°ˆæ¡ˆè·¯å¾‘æ­£ç¢º
   which uv
   cd /path/to/your/graphiti && pwd
   ```

2. **ç’°å¢ƒè®Šæ•¸**
   ```bash
   # æ¸¬è©¦ç’°å¢ƒè®Šæ•¸è¼‰å…¥
   echo $NEO4J_PASSWORD
   ```

3. **æœå‹™ç‹€æ…‹**
   ```bash
   # æª¢æŸ¥ Neo4j å’Œ Ollama æœå‹™
   neo4j status
   ollama list
   ```

4. **æ‰‹å‹•æ¸¬è©¦**
   ```bash
   # æ‰‹å‹•å•Ÿå‹•æœå‹™å™¨æ¸¬è©¦
   uv run python graphiti_mcp_server.py --transport stdio
   ```

## ğŸ”§ é…ç½®ç®¡ç†

### JSON é…ç½®æª”æ¡ˆç¯„ä¾‹

```json
{
  "ollama": {
    "model": "qwen2.5:7b",
    "base_url": "http://localhost:11434",
    "temperature": 0.1
  },
  "embedder": {
    "model": "nomic-embed-text:v1.5",
    "base_url": "http://localhost:11434",
    "dimensions": 768
  },
  "neo4j": {
    "uri": "bolt://localhost:7687",
    "user": "neo4j",
    "password": "your_neo4j_password"
  },
  "logging": {
    "level": "INFO",
    "file_path": "logs/graphiti_mcp.log",
    "backup_count": 30,
    "rotation_type": "time",
    "rotation_interval": "midnight"
  }
}
```

### ç’°å¢ƒè®Šæ•¸é…ç½® (`.env`)

```bash
# ======================
# Neo4j åœ–è³‡æ–™åº«é…ç½®
# ======================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password
NEO4J_DATABASE=graphiti-db

# ======================
# Ollama LLM é…ç½®
# ======================
# è¦†è“‹ OpenAI è¨­å®šï¼Œä½¿ç”¨ Ollama
OPENAI_API_KEY=ollama
OPENAI_BASE_URL=http://localhost:11434/v1

# æ¨è–¦çš„ LLM æ¨¡å‹
MODEL_NAME=qwen2.5:7b
SMALL_MODEL_NAME=qwen2.5:7b

# åµŒå…¥æ¨¡å‹ï¼ˆå¿…é ˆå®‰è£ï¼‰
EMBEDDER_MODEL_NAME=nomic-embed-text:v1.5

# ======================
# Graphiti é…ç½®
# ======================
# è¨˜æ†¶åˆ†çµ„ ID
GROUP_ID=your_group_id

# ä¸¦ç™¼é™åˆ¶ï¼ˆæœ¬åœ° LLM å»ºè­°è¼ƒä½å€¼ï¼‰
SEMAPHORE_LIMIT=3

# é—œé–‰é™æ¸¬
GRAPHITI_TELEMETRY_ENABLED=false

# ======================
# é€²éšé…ç½®
# ======================
# Ollama æœå‹™å™¨åœ°å€
OLLAMA_BASE_URL=http://localhost:11434

# LLM æº«åº¦è¨­å®šï¼ˆ0.0-1.0ï¼‰
LLM_TEMPERATURE=0.1

# ======================
# æ—¥èªŒé…ç½®
# ======================
# æ—¥èªŒæª”æ¡ˆè·¯å¾‘
LOG_FILE=logs/graphiti_mcp_server.log
LOG_LEVEL=INFO

# æ—¥èªŒè¼ªè½‰è¨­å®š
LOG_ROTATION_TYPE=time
LOG_ROTATION_INTERVAL=midnight
LOG_BACKUP_COUNT=30

# ======================
# å…¶ä»–é‡è¦è¨­å®š
# ======================
# Ollama æ¨¡å‹è¨­å®šï¼ˆå°æ‡‰ config.py ä¸­çš„è®Šæ•¸åï¼‰
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_TEMPERATURE=0.1
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:v1.5
OLLAMA_EMBEDDING_DIMENSIONS=768
OLLAMA_EMBEDDING_BASE_URL=http://localhost:11434

# æœå°‹é™åˆ¶
SEARCH_LIMIT=20

# åŠŸèƒ½é–‹é—œ
ENABLE_DEDUPLICATION=true
PYDANTIC_VALIDATION_FIXES=true

# ç›¸ä¼¼åº¦é–¾å€¼
COSINE_SIMILARITY_THRESHOLD=0.8
```

## ğŸ“‹ æ—¥èªŒæª”æ¡ˆç®¡ç†

### æ—¥èªŒæª”æ¡ˆå‘½åæ ¼å¼

ç³»çµ±ä½¿ç”¨æ¯æ—¥è¼ªè½‰çš„æ—¥èªŒæª”æ¡ˆï¼Œå‘½åæ ¼å¼å¦‚ä¸‹ï¼š

```
logs/
â”œâ”€â”€ graphiti_mcp_server_2025-09-17.log    # ç•¶å‰æ—¥æœŸçš„æ—¥èªŒ
â”œâ”€â”€ graphiti_mcp_server_2025-09-16.log    # å‰ä¸€å¤©çš„æ—¥èªŒ
â”œâ”€â”€ graphiti_mcp_server_2025-09-15.log    # æ›´æ—©çš„æ—¥èªŒ
â””â”€â”€ ... (ä¿ç•™30å¤©)
```

**å‘½åè¦å‰‡ï¼š**
- åŸºæœ¬æ ¼å¼ï¼š`graphiti_mcp_server_YYYY-MM-DD.log`
- æ¯æ—¥åˆå¤œè‡ªå‹•è¼ªè½‰
- ä¿ç•™30å¤©çš„æ­·å²æª”æ¡ˆ
- è‡ªå‹•æ¸…ç†éæœŸæª”æ¡ˆ

**è¨­å®šé¸é …ï¼š**
```bash
# æ—¥èªŒè¼ªè½‰é…ç½®
LOG_ROTATION_TYPE=time          # æ™‚é–“è¼ªè½‰
LOG_ROTATION_INTERVAL=midnight  # æ¯æ—¥åˆå¤œ
LOG_BACKUP_COUNT=30            # ä¿ç•™30å¤©
```

## ğŸ“š API åŠŸèƒ½

### ğŸ”§ ä½¿ç”¨å·¥å…·æŒ‡ä»¤

**é‡è¦ï¼š** åœ¨ä½¿ç”¨ Graphiti MCP å·¥å…·ä¹‹å‰ï¼Œè«‹å…ˆé–±è®€ **[ä½¿ç”¨å·¥å…·çš„æŒ‡ä»¤](docs/ä½¿ç”¨å·¥å…·çš„æŒ‡ä»¤.md)**ã€‚

è©²æ–‡ä»¶åŒ…å«ï¼š
- **æœç´¢å„ªå…ˆåŸå‰‡** - é–‹å§‹ä»»å‹™å‰å…ˆæœç´¢ç›¸é—œè³‡è¨Š
- **è³‡è¨Šå„²å­˜è¦ç¯„** - å¦‚ä½•æ­£ç¢ºå„²å­˜åå¥½ã€ç¨‹åºå’Œäº‹å¯¦
- **å·¥ä½œæµç¨‹æŒ‡å¼•** - æœ€ä½³å¯¦è¸å’Œæ³¨æ„äº‹é …
- **å·¥å…·ä½¿ç”¨ç¯„ä¾‹** - å¯¦éš›æ“ä½œç¤ºç¯„

**é—œéµåŸå‰‡ï¼š**
```
1. é–‹å§‹å‰å…ˆæœç´¢ï¼šsearch_memory_nodes + search_memory_facts
2. ç«‹å³å„²å­˜é‡è¦è³‡è¨Šï¼šadd_memory_simple
3. éµå¾ªç™¼ç¾çš„åå¥½å’Œç¨‹åº
4. ç¶­æŒè¨˜æ†¶çš„ä¸€è‡´æ€§å’Œå®Œæ•´æ€§
```

### è¨˜æ†¶ç®¡ç†

- **`add_memory_simple`** - æ–°å¢è¨˜æ†¶ç‰‡æ®µåˆ°çŸ¥è­˜åœ–è­œ
  ```json
  {
    "name": "å­¸ç¿’ Python",
    "episode_body": "ä»Šå¤©å­¸ç¿’äº† Python çš„é¡åˆ¥å’Œç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆæ¦‚å¿µ",
    "group_id": "å­¸ç¿’è¨˜éŒ„"
  }
  ```

- **`search_memory_nodes`** - æœå°‹è¨˜æ†¶ç¯€é» (å¯¦é«”)
  ```json
  {
    "query": "Python ç¨‹å¼è¨­è¨ˆ",
    "max_nodes": 10,
    "group_ids": ["å­¸ç¿’è¨˜éŒ„"]
  }
  ```

- **`search_memory_facts`** - æœå°‹è¨˜æ†¶äº‹å¯¦ (é—œä¿‚)
  ```json
  {
    "query": "ç¨‹å¼èªè¨€å­¸ç¿’",
    "max_facts": 10,
    "group_ids": ["å­¸ç¿’è¨˜éŒ„"]
  }
  ```

- **`get_episodes`** - ç²å–æœ€è¿‘çš„è¨˜æ†¶ç‰‡æ®µ
  ```json
  {
    "last_n": 5,
    "group_id": "å­¸ç¿’è¨˜éŒ„"
  }
  ```

### ç³»çµ±ç®¡ç†

- **`test_connection`** - æ¸¬è©¦ç³»çµ±é€£æ¥ç‹€æ…‹
- **`clear_graph`** - æ¸…é™¤æ‰€æœ‰åœ–è³‡æ–™åº«è³‡æ–™

## ğŸ” æœå°‹åŠŸèƒ½è©³è§£

### èªæ„æœå°‹ç‰¹è‰²

- **æ™ºèƒ½ç†è§£**: ä¸åªæ˜¯é—œéµå­—åŒ¹é…ï¼Œèƒ½ç†è§£æŸ¥è©¢çš„èªæ„
- **å‘é‡åµŒå…¥**: ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹é€²è¡Œå‘é‡ç›¸ä¼¼åº¦è¨ˆç®—
- **æ··åˆæœå°‹**: çµåˆé—œéµå­—æœå°‹å’Œå‘é‡æœå°‹çš„å„ªå‹¢
- **ç›¸é—œæ€§æ’åº**: è‡ªå‹•æŒ‰ç›¸é—œåº¦æ’åºæœå°‹çµæœ

### æœå°‹ç¯„ä¾‹

```bash
# æœå°‹ç¨‹å¼ç›¸é—œè¨˜æ†¶
query: "Python å­¸ç¿’"
# èƒ½æ‰¾åˆ°: "ç¨‹å¼è¨­è¨ˆèª²ç¨‹", "ç·¨ç¨‹æŠ€å·§", "é–‹ç™¼ç¶“é©—" ç­‰ç›¸é—œå…§å®¹

# æœå°‹å·¥ä½œç›¸é—œäº‹å¯¦
query: "å°ˆæ¡ˆç®¡ç†"
# èƒ½æ‰¾åˆ°: "åœ˜éšŠå”ä½œ", "é€²åº¦è¿½è¹¤", "éœ€æ±‚åˆ†æ" ç­‰é—œè¯é—œä¿‚
```

## ğŸ“Š ç›£æ§å’Œæ—¥èªŒ

### çµæ§‹åŒ–æ—¥èªŒ

#### æ—¥èªŒæª”æ¡ˆå‘½åè¦å‰‡

ç³»çµ±ä½¿ç”¨æŒ‰æ—¥æœŸåˆ†å‰²çš„æ—¥èªŒæª”æ¡ˆï¼Œé¿å…å–®ä¸€æª”æ¡ˆéå¤§ï¼š

```bash
logs/
â”œâ”€â”€ graphiti_mcp_2025-01-15.log  # ä»Šå¤©çš„æ—¥èªŒ
â”œâ”€â”€ graphiti_mcp_2025-01-14.log  # æ˜¨å¤©çš„æ—¥èªŒ
â”œâ”€â”€ graphiti_mcp_2025-01-13.log  # å‰å¤©çš„æ—¥èªŒ
â””â”€â”€ ...                          # ä¿ç•™ 30 å¤©

# æŸ¥çœ‹ä»Šå¤©çš„æ—¥èªŒ
tail -f logs/graphiti_mcp_$(date +%Y-%m-%d).log

# æŸ¥çœ‹æ‰€æœ‰æ—¥èªŒ
tail -f logs/graphiti_mcp_*.log

# æœå°‹ç‰¹å®šæ“ä½œï¼ˆæ‰€æœ‰æ—¥æœŸï¼‰
grep "add_memory" logs/graphiti_mcp_*.log

# æœå°‹ä»Šå¤©çš„ç‰¹å®šæ“ä½œ
grep "add_memory" logs/graphiti_mcp_$(date +%Y-%m-%d).log
```

#### æ—¥èªŒè¼ªè½‰é…ç½®

åœ¨é…ç½®æª”æ¡ˆä¸­å¯ä»¥èª¿æ•´æ—¥èªŒè¼ªè½‰è¨­å®šï¼š

```json
{
  "logging": {
    "rotation_type": "time",      // "time" æˆ– "size"
    "rotation_interval": "midnight", // è¼ªè½‰æ™‚é–“é»
    "backup_count": 30,           // ä¿ç•™æª”æ¡ˆæ•¸é‡
    "max_file_size": 10485760     // å¤§å°è¼ªè½‰æ™‚çš„æª”æ¡ˆå¤§å°é™åˆ¶
  }
}
```

**æ™‚é–“è¼ªè½‰é¸é …ï¼š**
- `midnight` - æ¯å¤©åˆå¤œè¼ªè½‰ (é è¨­)
- `H` - æ¯å°æ™‚è¼ªè½‰
- `D` - æ¯å¤©è¼ªè½‰
- `W0`-`W6` - æ¯é€±ç‰¹å®šæ—¥æœŸè¼ªè½‰

**å¤§å°è¼ªè½‰ï¼š**
- æª”æ¡ˆé”åˆ°æŒ‡å®šå¤§å°æ™‚è‡ªå‹•è¼ªè½‰
- é©åˆé«˜é »ä½¿ç”¨çš„å ´æ™¯

### æ€§èƒ½ç›£æ§

- â±ï¸ **æ“ä½œåŸ·è¡Œæ™‚é–“è¿½è¹¤**
- ğŸ“ˆ **è¨˜æ†¶æ·»åŠ æ€§èƒ½æŒ‡æ¨™**
- ğŸ” **Neo4j æŸ¥è©¢æ€§èƒ½åˆ†æ**
- ğŸ§² **åµŒå…¥ç”Ÿæˆæ•ˆèƒ½ç›£æ§**

## ğŸ§ª æ¸¬è©¦

```bash
# é‹è¡Œæ ¸å¿ƒæ¸¬è©¦
uv run python -m pytest tests/

# é‹è¡Œå®Œæ•´é›†æˆæ¸¬è©¦
uv run python tests/comprehensive_test.py

# é‹è¡Œæ€§èƒ½æ¸¬è©¦
uv run python tests/model_performance_test.py
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **Neo4j é€£æ¥å¤±æ•—**
   - æª¢æŸ¥ Neo4j æœå‹™æ˜¯å¦é‹è¡Œ
   - ç¢ºèªå¯†ç¢¼å’Œé€£æ¥è¨­å®šæ­£ç¢º

2. **Ollama é€£æ¥å¤±æ•—**
   - ç¢ºèª Ollama æœå‹™é‹è¡Œ: `ollama serve`
   - æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰: `ollama pull qwen2.5:7b`

3. **æœå°‹ç„¡çµæœ**
   - ç¢ºèªå·²æœ‰è¨˜æ†¶è³‡æ–™
   - æª¢æŸ¥ group_ids ç¯©é¸æ¢ä»¶
   - æŸ¥çœ‹æ—¥èªŒæª”æ¡ˆäº†è§£è©³ç´°éŒ¯èª¤

### æ—¥èªŒåˆ†æ

```bash
# æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒ
grep "ERROR\|WARN" logs/graphiti_mcp.log

# ç›£æ§æ€§èƒ½
grep "duration" logs/graphiti_mcp.log
```

## ğŸ› ï¸ é–‹ç™¼å·¥å…·

```bash
# æ€§èƒ½è¨ºæ–·
uv run python tools/performance_diagnose.py

# çµæ§‹æª¢æŸ¥
uv run python tools/inspect_schema.py

# ç‹€æ…‹å ±å‘Š
uv run python tools/final_status_report.py
```

## ğŸ”„ èˆ‡åŸç‰ˆå·®ç•°

### å„ªåŒ–æ”¹é€²

- ğŸ—ï¸ **ä¼æ¥­ç´šæ¶æ§‹**: æ¨¡çµ„åŒ–è¨­è¨ˆå’Œçµæ§‹åŒ–ç•°å¸¸è™•ç†
- ğŸ‡¹ğŸ‡¼ **å®Œæ•´ä¸­æ–‡åŒ–**: API å›æ‡‰å’Œä½¿ç”¨è€…ä»‹é¢å…¨ä¸­æ–‡
- âš¡ **æ€§èƒ½å„ªåŒ–**: æ™ºèƒ½é…ç½®ç®¡ç†å’Œé€£æ¥æ± 
- ğŸ“Š **å®Œæ•´ç›£æ§**: çµæ§‹åŒ–æ—¥èªŒå’Œæ€§èƒ½è¿½è¹¤
- ğŸ” **æ”¹é€²æœå°‹**: ä½¿ç”¨æ­£ç¢ºçš„ Graphiti API é€²è¡Œèªæ„æœå°‹

### ä¿æŒç›¸å®¹

- âœ… **API ç›¸å®¹**: èˆ‡åŸç‰ˆ MCP å·¥å…·åƒæ•¸å®Œå…¨ç›¸å®¹
- âœ… **åŠŸèƒ½å®Œæ•´**: ä¿ç•™æ‰€æœ‰åŸç‰ˆæ ¸å¿ƒåŠŸèƒ½
- âœ… **é…ç½®å½ˆæ€§**: æ”¯æ´ JSON é…ç½®å’Œç’°å¢ƒè®Šæ•¸

## ğŸ“œ æˆæ¬Š

MIT License

## ğŸ¤ è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**å°ˆç‚ºå°ç£é–‹ç™¼è€…æ‰“é€ çš„æœ¬åœ°åŒ–çŸ¥è­˜åœ–è­œè§£æ±ºæ–¹æ¡ˆ** ğŸ‡¹ğŸ‡¼