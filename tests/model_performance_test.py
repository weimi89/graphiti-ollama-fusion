#!/usr/bin/env python3
"""
æ¨¡å‹æ•ˆèƒ½æ¸¬è©¦ - æ¸¬è©¦ä¸åŒæ¨¡å‹çµ„åˆçš„æ•ˆèƒ½è¡¨ç¾
é©åˆä¸åŒç¡¬é«”é…ç½®çš„ç”¨æˆ¶
"""
import asyncio
import time
import os
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class ModelConfig:
    name: str
    llm_model: str
    embed_model: str
    description: str
    hardware_requirement: str

# å®šç¾©ä¸åŒçš„æ¨¡å‹çµ„åˆ
MODEL_CONFIGS = [
    ModelConfig(
        name="é«˜æ•ˆèƒ½çµ„åˆ",
        llm_model="qwen2.5:14b",
        embed_model="nomic-embed-text:v1.5",
        description="æœ€ä½³æ•ˆæœï¼Œéœ€è¦å¼·å¤§ç¡¬é«”",
        hardware_requirement="16GB+ RAM, GPU æ¨è–¦"
    ),
    ModelConfig(
        name="å¹³è¡¡çµ„åˆ",
        llm_model="qwen2.5:7b",
        embed_model="nomic-embed-text:v1.5",
        description="å¹³è¡¡æ•ˆæœå’Œè³‡æºä½¿ç”¨",
        hardware_requirement="8GB+ RAM"
    ),
    ModelConfig(
        name="è¼•é‡çµ„åˆ",
        llm_model="llama3.2:3b",
        embed_model="all-minilm:l6-v2",
        description="è³‡æºå‹å¥½ï¼Œé©åˆä½é…ç¡¬é«”",
        hardware_requirement="4GB+ RAM"
    ),
    ModelConfig(
        name="æ¥µè¼•é‡çµ„åˆ",
        llm_model="phi3.5:3.8b",
        embed_model="all-minilm:l6-v2",
        description="æœ€å°‘è³‡æºä½¿ç”¨",
        hardware_requirement="2GB+ RAM"
    )
]

class ModelPerformanceTester:
    def __init__(self):
        self.results = []

    async def test_model_availability(self, model_name: str) -> bool:
        """æ¸¬è©¦æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
                async with session.get('http://localhost:11434/api/tags') as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [m['name'] for m in data.get('models', [])]
                        return model_name in models
            return False
        except:
            return False

    async def test_model_speed(self, config: ModelConfig) -> Dict:
        """æ¸¬è©¦æ¨¡å‹çš„é€Ÿåº¦å’ŒéŸ¿æ‡‰"""
        print(f"\nğŸ§ª æ¸¬è©¦ {config.name} ({config.llm_model})")
        print(f"   ç¡¬é«”éœ€æ±‚: {config.hardware_requirement}")

        # æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§
        llm_available = await self.test_model_availability(config.llm_model)
        embed_available = await self.test_model_availability(config.embed_model)

        if not llm_available:
            print(f"   âŒ LLM æ¨¡å‹ {config.llm_model} ä¸å¯ç”¨")
            return {
                'config': config,
                'available': False,
                'error': f'LLM æ¨¡å‹ {config.llm_model} ä¸å¯ç”¨'
            }

        if not embed_available:
            print(f"   âŒ åµŒå…¥æ¨¡å‹ {config.embed_model} ä¸å¯ç”¨")
            return {
                'config': config,
                'available': False,
                'error': f'åµŒå…¥æ¨¡å‹ {config.embed_model} ä¸å¯ç”¨'
            }

        print(f"   âœ… æ¨¡å‹å¯ç”¨ï¼Œé–‹å§‹æ•ˆèƒ½æ¸¬è©¦...")

        try:
            # è¨­å®šç’°å¢ƒè®Šæ•¸
            os.environ['MODEL_NAME'] = config.llm_model
            os.environ['EMBEDDER_MODEL_NAME'] = config.embed_model

            # é‡æ–°å°å…¥ä»¥ä½¿ç”¨æ–°è¨­å®š
            import importlib
            import sys

            # æ¸…é™¤ç›¸é—œæ¨¡çµ„
            modules_to_reload = []
            for module_name in list(sys.modules.keys()):
                if any(name in module_name for name in ['ollama', 'graphiti_mcp_server']):
                    modules_to_reload.append(module_name)

            for module_name in modules_to_reload:
                if module_name in sys.modules:
                    del sys.modules[module_name]

            # é‡æ–°å°å…¥
            from graphiti_mcp_server import (
                add_memory_simple, AddMemoryArgs,
                search_memory_nodes, SearchNodesArgs,
                initialize_graphiti
            )

            # åˆå§‹åŒ–æ¸¬è©¦
            start_time = time.time()
            await initialize_graphiti()
            init_time = time.time() - start_time

            # è¨˜æ†¶æ·»åŠ æ¸¬è©¦
            start_time = time.time()
            result = await add_memory_simple(AddMemoryArgs(
                name=f"æ•ˆèƒ½æ¸¬è©¦-{config.name}",
                episode_body=f"é€™æ˜¯ä½¿ç”¨ {config.llm_model} æ¨¡å‹çš„æ•ˆèƒ½æ¸¬è©¦è¨˜æ†¶",
                group_id=f"perf_test_{config.name.replace(' ', '_')}"
            ))
            memory_time = time.time() - start_time

            # æœç´¢æ¸¬è©¦
            start_time = time.time()
            search_result = await search_memory_nodes(SearchNodesArgs(
                query="æ•ˆèƒ½æ¸¬è©¦",
                max_nodes=5
            ))
            search_time = time.time() - start_time

            test_result = {
                'config': config,
                'available': True,
                'init_time': init_time,
                'memory_time': memory_time,
                'search_time': search_time,
                'memory_success': not result.get('error', False),
                'search_success': not search_result.get('error', False),
                'nodes_extracted': result.get('nodes_extracted', 0),
                'edges_created': result.get('edges_created', 0),
                'nodes_found': len(search_result.get('nodes', []))
            }

            print(f"   ğŸ“Š åˆå§‹åŒ–: {init_time:.2f}s")
            print(f"   ğŸ“Š è¨˜æ†¶æ·»åŠ : {memory_time:.2f}s")
            print(f"   ğŸ“Š æœç´¢: {search_time:.2f}s")
            print(f"   ğŸ“Š å¯¦é«”æå–: {test_result['nodes_extracted']} å€‹")
            print(f"   ğŸ“Š é—œä¿‚å‰µå»º: {test_result['edges_created']} å€‹")

            return test_result

        except Exception as e:
            print(f"   âŒ æ¸¬è©¦å¤±æ•—: {e}")
            return {
                'config': config,
                'available': True,
                'error': str(e)
            }

    async def run_performance_tests(self):
        """é‹è¡Œæ‰€æœ‰æ¨¡å‹æ•ˆèƒ½æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹æ¨¡å‹æ•ˆèƒ½æ¸¬è©¦")
        print("=" * 60)
        print("æ¸¬è©¦ç›®æ¨™ï¼šæ‰¾å‡ºæœ€é©åˆä¸åŒç¡¬é«”é…ç½®çš„æ¨¡å‹çµ„åˆ")
        print("=" * 60)

        for config in MODEL_CONFIGS:
            result = await self.test_model_speed(config)
            self.results.append(result)

            # åœ¨æ¸¬è©¦é–“ç¨ä½œåœé “
            await asyncio.sleep(2)

        # ç”Ÿæˆå ±å‘Š
        self.generate_report()

    def generate_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¨¡å‹æ•ˆèƒ½æ¸¬è©¦å ±å‘Š")
        print("=" * 60)

        # æŒ‰å¯ç”¨æ€§åˆ†é¡
        available_results = [r for r in self.results if r.get('available', False) and 'error' not in r]
        unavailable_results = [r for r in self.results if not r.get('available', False) or 'error' in r]

        if available_results:
            print("\nâœ… å¯ç”¨æ¨¡å‹çµ„åˆ:")
            # æŒ‰ç¸½æ™‚é–“æ’åºï¼ˆåˆå§‹åŒ– + è¨˜æ†¶æ·»åŠ ï¼‰
            available_results.sort(key=lambda x: x.get('init_time', 999) + x.get('memory_time', 999))

            for i, result in enumerate(available_results, 1):
                config = result['config']
                total_time = result.get('init_time', 0) + result.get('memory_time', 0)

                print(f"\n{i}. {config.name} ğŸ†" if i == 1 else f"\n{i}. {config.name}")
                print(f"   æ¨¡å‹: {config.llm_model} + {config.embed_model}")
                print(f"   ç¡¬é«”éœ€æ±‚: {config.hardware_requirement}")
                print(f"   ç¸½è€—æ™‚: {total_time:.2f}s (åˆå§‹åŒ–: {result.get('init_time', 0):.2f}s + è¨˜æ†¶: {result.get('memory_time', 0):.2f}s)")
                print(f"   åŠŸèƒ½æ€§: å¯¦é«”æå– {result.get('nodes_extracted', 0)} å€‹, é—œä¿‚ {result.get('edges_created', 0)} å€‹")

                # æ€§èƒ½è©•ç´š
                if total_time < 3:
                    print(f"   âš¡ æ•ˆèƒ½è©•ç´š: æ¥µä½³")
                elif total_time < 8:
                    print(f"   ğŸŸ¢ æ•ˆèƒ½è©•ç´š: è‰¯å¥½")
                elif total_time < 15:
                    print(f"   ğŸŸ¡ æ•ˆèƒ½è©•ç´š: å¯æ¥å—")
                else:
                    print(f"   ğŸ”´ æ•ˆèƒ½è©•ç´š: éœ€è¦å„ªåŒ–")

        if unavailable_results:
            print(f"\nâŒ ä¸å¯ç”¨æ¨¡å‹çµ„åˆ ({len(unavailable_results)} å€‹):")
            for result in unavailable_results:
                config = result['config']
                error = result.get('error', 'æœªçŸ¥éŒ¯èª¤')
                print(f"   â€¢ {config.name}: {error}")

        # å»ºè­°
        print(f"\nğŸ’¡ å»ºè­°:")
        if available_results:
            best_config = available_results[0]['config']
            print(f"   â€¢ æ¨è–¦çµ„åˆ: {best_config.name}")
            print(f"   â€¢ é©åˆç¡¬é«”: {best_config.hardware_requirement}")
            print(f"   â€¢ ä½¿ç”¨æ–¹å¼: åœ¨ .env ä¸­è¨­å®š:")
            print(f"     MODEL_NAME={best_config.llm_model}")
            print(f"     EMBEDDER_MODEL_NAME={best_config.embed_model}")
        else:
            print("   â€¢ è«‹å…ˆå®‰è£å¿…è¦çš„ Ollama æ¨¡å‹")
            print("   â€¢ å»ºè­°åŸ·è¡Œ: ollama pull qwen2.5:7b && ollama pull nomic-embed-text:v1.5")

async def main():
    tester = ModelPerformanceTester()
    await tester.run_performance_tests()

if __name__ == "__main__":
    asyncio.run(main())