#!/usr/bin/env python3
"""
å„ªåŒ–çš„ Ollama Graphiti å®¢æˆ¶ç«¯
å°ˆé–€è™•ç† Pydantic æ¨¡å‹å’Œ JSON éŸ¿æ‡‰
"""

import asyncio
import os
import sys
from datetime import datetime, timezone
from typing import List, Optional, Any, Dict, Union
from dotenv import load_dotenv
import aiohttp
import json
from pydantic import BaseModel

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ç¢ºä¿å°å…¥è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from graphiti_core import Graphiti
from graphiti_core.llm_client.client import LLMClient
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.cross_encoder.client import CrossEncoderClient
from ollama_embedder import OllamaEmbedder


class OptimizedOllamaClient(LLMClient):
    """
    å„ªåŒ–çš„ Ollama å®¢æˆ¶ç«¯ï¼Œæ›´å¥½åœ°è™•ç† JSON å’Œ Pydantic æ¨¡å‹
    """

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.base_url = config.base_url or "http://localhost:11434"
        # ä½¿ç”¨å‚³å…¥çš„ modelï¼Œå¦‚æœæ²’æœ‰æ‰ç”¨é»˜èªå€¼
        self.model = config.model if config.model else "llama3.2:3b"
        self.temperature = config.temperature if config.temperature is not None else 0.0
        print(f"    ä½¿ç”¨æ¨¡å‹: {self.model}")

    async def _generate_response(
        self,
        messages: List[Any],
        response_model: Optional[Any] = None,
        **kwargs
    ) -> Any:
        """å¯¦ç¾æŠ½è±¡æ–¹æ³• _generate_response"""
        return await self.generate_response(messages, response_model)

    async def _make_request(
        self,
        messages: List[Dict[str, str]],
        json_mode: bool = False
    ) -> str:
        """ç™¼é€è«‹æ±‚åˆ° Ollama API"""
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "temperature": self.temperature,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": 4096
                }
            }

            if json_mode:
                payload["format"] = "json"

            url = f"{self.base_url}/api/chat"

            async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=120)) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("message", {}).get("content", "")
                else:
                    error_text = await response.text()
                    print(f"âŒ Ollama éŒ¯èª¤: {error_text}")
                    return ""

    def _extract_json_from_response(self, response: str) -> Dict:
        """å¾éŸ¿æ‡‰ä¸­æå– JSON"""
        # å˜—è©¦ç›´æ¥è§£æ
        try:
            return json.loads(response)
        except:
            pass

        # å˜—è©¦æå– JSON å¡Š
        import re
        patterns = [
            r'```json\s*(.*?)\s*```',
            r'```\s*(.*?)\s*```',
            r'\{[^}]*\}',
            r'\{.*\}',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, response, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match)
                except:
                    continue

        # å¦‚æœéƒ½å¤±æ•—ï¼Œè¿”å›ç©ºå­—å…¸
        return {}

    async def generate_response(
        self,
        messages: List[Any],
        response_model: Optional[Any] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Any:
        """ç”ŸæˆéŸ¿æ‡‰"""
        # è½‰æ›æ¶ˆæ¯æ ¼å¼
        ollama_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                ollama_messages.append(msg)
            else:
                ollama_messages.append({
                    "role": getattr(msg, 'role', 'user'),
                    "content": getattr(msg, 'content', str(msg))
                })

        # å¦‚æœéœ€è¦çµæ§‹åŒ–è¼¸å‡º
        if response_model:
            # ç‚º Pydantic æ¨¡å‹ç”Ÿæˆ JSON schema æç¤º
            if hasattr(response_model, '__annotations__'):
                # ç²å–å­—æ®µä¿¡æ¯
                fields = {}
                for field_name, field_type in response_model.__annotations__.items():
                    if hasattr(field_type, '__origin__'):
                        # è™•ç†æ³›å‹é¡å‹
                        if field_type.__origin__ == list:
                            fields[field_name] = "array"
                        elif field_type.__origin__ == dict:
                            fields[field_name] = "object"
                        else:
                            fields[field_name] = "string"
                    else:
                        # ç°¡å–®é¡å‹
                        type_map = {
                            str: "string",
                            int: "number",
                            float: "number",
                            bool: "boolean",
                            list: "array",
                            dict: "object"
                        }
                        fields[field_name] = type_map.get(field_type, "string")

                # æ§‹å»º JSON æ ¼å¼æç¤º
                schema_hint = f"Return a JSON object with these fields: {json.dumps(fields)}"

                # æ·»åŠ æ›´è©³ç´°çš„ç³»çµ±æç¤º
                system_prompt = f"""You are an entity extraction assistant. Your task is to extract ALL entities and relationships from text.

IMPORTANT:
1. Extract ALL entities mentioned in the text (people, organizations, technologies, concepts)
2. Each entity should have a unique name
3. Entity types: 0=Person, 1=Organization, 2=Technology, 3=Concept, 4=Place, 5=Other
4. Include observations about each entity
5. Respond with valid JSON only

{schema_hint}

Example entities to look for:
- People names (type 0)
- Company/organization names (type 1)
- Technologies/tools/frameworks (type 2)
- Concepts/methods (type 3)
- Locations (type 4)
- Other entities (type 5)"""

                ollama_messages.append({
                    "role": "system",
                    "content": system_prompt
                })

            # ç™¼é€è«‹æ±‚
            response_text = await self._make_request(ollama_messages, json_mode=True)

            # è§£æéŸ¿æ‡‰
            if response_text:
                json_data = self._extract_json_from_response(response_text)

                # ä¿®æ­£ä¸­æ–‡/è‹±æ–‡å­—æ®µåæ˜ å°„
                entities_data = None
                if 'å¯¦é«”' in json_data:
                    entities_data = json_data['å¯¦é«”']
                    json_data['extracted_entities'] = entities_data
                elif 'extracted_entities' in json_data:
                    entities_data = json_data['extracted_entities']

                # èª¿è©¦è¼¸å‡ºå’Œå­—æ®µä¿®æ­£
                if entities_data is not None:
                    print(f"\nğŸ” èª¿è©¦: æå–çš„å¯¦é«”æ•¸é‡ = {len(entities_data) if isinstance(entities_data, list) else 0}")

                    # ç¢ºä¿ entities_data æ˜¯åˆ—è¡¨ä¸”åŒ…å«æœ‰æ•ˆå¯¦é«”
                    if isinstance(entities_data, list):
                        # éæ¿¾ç©ºå­—ç¬¦ä¸²æˆ–ç„¡æ•ˆå¯¦é«”
                        valid_entities = []
                        for i, entity in enumerate(entities_data):
                            if isinstance(entity, dict):
                                # ç«‹å³ä¿®æ­£ entity_name -> name
                                if 'entity_name' in entity and 'name' not in entity:
                                    entity['name'] = entity.pop('entity_name')
                                valid_entities.append(entity)
                                print(f"  å¯¦é«” {i}: {entity}")
                            elif isinstance(entity, str) and entity.strip():
                                # å°‡å­—ç¬¦ä¸²è½‰æ›ç‚ºå¯¦é«”å­—å…¸
                                entity_dict = {
                                    'name': entity.strip(),
                                    'labels': ['Entity'],
                                    'summary': entity.strip()
                                }
                                valid_entities.append(entity_dict)
                                print(f"  å¯¦é«” {i}: {entity} (è½‰æ›ç‚ºå­—å…¸)")

                        # æ›´æ–°å¯¦é«”åˆ—è¡¨
                        json_data['extracted_entities'] = valid_entities
                    else:
                        print(f"  è­¦å‘Šï¼šå¯¦é«”æ•¸æ“šä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(entities_data)}")
                        # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œå˜—è©¦è½‰æ›
                        if isinstance(entities_data, str) and entities_data.strip():
                            entity_dict = {
                                'name': entities_data.strip(),
                                'labels': ['Entity'],
                                'summary': entities_data.strip()
                            }
                            json_data['extracted_entities'] = [entity_dict]
                            print(f"  å·²è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼: {entity_dict}")

                # å˜—è©¦å‰µå»º Pydantic æ¨¡å‹å¯¦ä¾‹
                if hasattr(response_model, 'model_validate'):
                    try:
                        # ä¿®æ­£å­—æ®µåç¨±æ˜ å°„å•é¡Œ
                        # å¦‚æœ json_data åŒ…å« extracted_entities å­—æ®µ
                        if 'extracted_entities' in json_data and isinstance(json_data['extracted_entities'], list):
                            for entity in json_data['extracted_entities']:
                                # å°‡ entity_name æ˜ å°„åˆ° name
                                if 'entity_name' in entity and 'name' not in entity:
                                    entity['name'] = entity.pop('entity_name')
                                # å°‡ entity_type_name æ˜ å°„åˆ° entity_type
                                if 'entity_type_name' in entity and 'entity_type' not in entity:
                                    entity['entity_type'] = entity.pop('entity_type_name')
                                # ç¢ºä¿æœ‰ entity_summary
                                if 'entity_summary' not in entity:
                                    entity['entity_summary'] = entity.get('description', entity.get('name', ''))
                                # ç¢ºä¿æœ‰ observationsï¼ˆå¿…éœ€å­—æ®µï¼‰
                                if 'observations' not in entity:
                                    entity['observations'] = [entity.get('description', f"Related to {entity.get('name', 'entity')}")]
                                # ç¢ºä¿æœ‰ entity_type_idï¼Œé è¨­ç‚º 0
                                if 'entity_type_id' not in entity:
                                    entity['entity_type_id'] = 0
                                else:
                                    # å¼·åˆ¶è¨­ç‚º 0 ä»¥é¿å… index out of range
                                    entity['entity_type_id'] = 0
                                # ç§»é™¤ä¸éœ€è¦çš„å­—æ®µ
                                for key in ['description', 'score', 'mentioned', 'speaker']:
                                    entity.pop(key, None)

                        # è™•ç† edges çš„å­—æ®µæ˜ å°„ï¼ˆä¿®æ­£ source_id -> source_entity_idï¼‰
                        if 'edges' in json_data and isinstance(json_data['edges'], list):
                            for i, edge in enumerate(json_data['edges']):
                                # ä¿®æ­£å„ç¨®å¯èƒ½çš„ ID å­—æ®µåç¨±
                                if 'source_id' in edge and 'source_entity_id' not in edge:
                                    edge['source_entity_id'] = edge.pop('source_id')
                                elif 'subject_id' in edge and 'source_entity_id' not in edge:
                                    edge['source_entity_id'] = edge.pop('subject_id')
                                elif 'source_entity_id' not in edge:
                                    # å¦‚æœæ²’æœ‰ source_entity_idï¼Œå˜—è©¦å¾ä½ç½®æ¨æ–·
                                    edge['source_entity_id'] = 0  # é»˜èªç¬¬ä¸€å€‹å¯¦é«”

                                if 'target_id' in edge and 'target_entity_id' not in edge:
                                    edge['target_entity_id'] = edge.pop('target_id')
                                elif 'object_id' in edge and 'target_entity_id' not in edge:
                                    edge['target_entity_id'] = edge.pop('object_id')
                                elif 'target_entity_id' not in edge:
                                    # å¦‚æœæ²’æœ‰ target_entity_idï¼Œå˜—è©¦å¾ä½ç½®æ¨æ–·
                                    edge['target_entity_id'] = 1 if len(json_data.get('extracted_entities', [])) > 1 else 0

                                # è™•ç†é—œä¿‚é¡å‹å­—æ®µ
                                if 'relationship' in edge and 'relation_type' not in edge:
                                    edge['relation_type'] = edge.pop('relationship')
                                elif 'predicate' in edge and 'relation_type' not in edge:
                                    edge['relation_type'] = edge.pop('predicate')
                                elif 'relation_type' not in edge:
                                    edge['relation_type'] = 'RELATES_TO'

                                # å°‡ relation_type ä¹Ÿæ˜ å°„åˆ° fact å­—æ®µ
                                if 'fact' not in edge:
                                    edge['fact'] = edge.get('relation_type', 'relates to')

                                # ä¿®å¾©IDå­—æ®µ - å°‡å­—ç¬¦ä¸²IDè½‰æ›ç‚ºæ•´æ•¸
                                if 'source_entity_id' in edge:
                                    try:
                                        if isinstance(edge['source_entity_id'], str):
                                            # å˜—è©¦å¾ "ENTITY_0" æ ¼å¼æå–æ•¸å­—
                                            if edge['source_entity_id'].startswith('ENTITY_'):
                                                edge['source_entity_id'] = int(edge['source_entity_id'].replace('ENTITY_', ''))
                                            else:
                                                edge['source_entity_id'] = int(edge['source_entity_id'])
                                        elif edge['source_entity_id'] is None:
                                            edge['source_entity_id'] = 0  # è™•ç† None å€¼
                                    except (ValueError, AttributeError, TypeError):
                                        edge['source_entity_id'] = 0  # é»˜èªå€¼

                                if 'target_entity_id' in edge:
                                    try:
                                        if isinstance(edge['target_entity_id'], str):
                                            # å˜—è©¦å¾ "ENTITY_0" æ ¼å¼æå–æ•¸å­—
                                            if edge['target_entity_id'].startswith('ENTITY_'):
                                                edge['target_entity_id'] = int(edge['target_entity_id'].replace('ENTITY_', ''))
                                            else:
                                                edge['target_entity_id'] = int(edge['target_entity_id'])
                                        elif edge['target_entity_id'] is None:
                                            edge['target_entity_id'] = 1  # è™•ç† None å€¼
                                    except (ValueError, AttributeError, TypeError):
                                        edge['target_entity_id'] = 1  # é»˜èªå€¼ï¼ˆé¿å…èˆ‡sourceç›¸åŒï¼‰

                                # ç¢ºä¿å¿…è¦å­—æ®µ
                                if 'fact' not in edge:
                                    edge['fact'] = edge.get('relation_type', 'relates to')
                                if 'fact_embedding' not in edge:
                                    edge['fact_embedding'] = None
                                if 'valid_at' not in edge:
                                    edge['valid_at'] = None
                                if 'invalid_at' not in edge:
                                    edge['invalid_at'] = None

                        # è™•ç† NodeResolutions çš„å­—æ®µæ˜ å°„
                        if 'entity_resolutions' in json_data and isinstance(json_data['entity_resolutions'], list):
                            for resolution in json_data['entity_resolutions']:
                                # ä¿®æ­£ duplicate_idx å­—æ®µ
                                if 'duplication_idx' in resolution:
                                    resolution['duplicate_idx'] = resolution.pop('duplication_idx')
                                # ç¢ºä¿æœ‰å¿…è¦çš„å­—æ®µ
                                if 'duplicate_idx' not in resolution:
                                    resolution['duplicate_idx'] = -1
                                if 'additional_duplicates' not in resolution:
                                    resolution['additional_duplicates'] = []

                        # è™•ç†ç¼ºå¤±çš„å­—æ®µ
                        for field_name in response_model.__annotations__.keys():
                            if field_name not in json_data:
                                # ç‚ºç¼ºå¤±çš„å­—æ®µæä¾›é»˜èªå€¼
                                field_type = response_model.__annotations__[field_name]
                                if hasattr(field_type, '__origin__') and field_type.__origin__ == list:
                                    json_data[field_name] = []
                                elif hasattr(field_type, '__origin__') and field_type.__origin__ == dict:
                                    json_data[field_name] = {}
                                elif field_type == bool:
                                    json_data[field_name] = False
                                elif field_type == int or field_type == float:
                                    json_data[field_name] = 0
                                else:
                                    json_data[field_name] = ""

                        validated = response_model.model_validate(json_data)
                        # ç¸½æ˜¯è¿”å›å­—å…¸å½¢å¼ä»¥ç¢ºä¿å…¼å®¹æ€§
                        return validated.model_dump()
                    except Exception as e:
                        print(f"âš ï¸ Pydantic é©—è­‰å¤±æ•—: {e}")
                        # å˜—è©¦å‰µå»ºä¸€å€‹æœ€å°æœ‰æ•ˆå¯¦ä¾‹
                        try:
                            minimal_data = {}
                            for field_name in response_model.__annotations__.keys():
                                field_type = response_model.__annotations__[field_name]
                                if hasattr(field_type, '__origin__') and field_type.__origin__ == list:
                                    minimal_data[field_name] = []
                                elif hasattr(field_type, '__origin__') and field_type.__origin__ == dict:
                                    minimal_data[field_name] = {}
                                else:
                                    minimal_data[field_name] = json_data.get(field_name, "")
                            validated = response_model.model_validate(minimal_data)
                            # ç¸½æ˜¯è¿”å›å­—å…¸å½¢å¼ä»¥ç¢ºä¿å…¼å®¹æ€§
                            return validated.model_dump()
                        except:
                            pass

            # å¦‚æœæ‰€æœ‰éƒ½å¤±æ•—ï¼Œå‰µå»ºç©ºå¯¦ä¾‹
            if hasattr(response_model, 'model_validate'):
                try:
                    empty_data = {
                        field_name: [] if 'list' in str(field_type).lower() else {}
                        if 'dict' in str(field_type).lower() else ""
                        for field_name, field_type in response_model.__annotations__.items()
                    }
                    validated = response_model.model_validate(empty_data)
                    # ç¸½æ˜¯è¿”å›å­—å…¸å½¢å¼ä»¥ç¢ºä¿å…¼å®¹æ€§
                    return validated.model_dump()
                except:
                    pass

            return None

        else:
            # ç´”æ–‡æœ¬éŸ¿æ‡‰
            return await self._make_request(ollama_messages)

    async def generate_response_with_retry(
        self,
        messages: List[Any],
        response_model: Optional[Any] = None,
        max_attempts: int = 3,
    ) -> Any:
        """å¸¶é‡è©¦çš„éŸ¿æ‡‰ç”Ÿæˆ"""
        for attempt in range(max_attempts):
            try:
                result = await self.generate_response(messages, response_model)
                if result:
                    return result
            except Exception as e:
                if attempt < max_attempts - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—: {e}")

        # è¿”å›é»˜èªå€¼
        if response_model and hasattr(response_model, 'model_validate'):
            try:
                empty_data = {
                    field_name: [] if 'list' in str(field_type).lower() else ""
                    for field_name, field_type in response_model.__annotations__.items()
                }
                return response_model.model_validate(empty_data)
            except:
                pass
        return None


class SimpleCrossEncoder(CrossEncoderClient):
    """ç°¡åŒ–çš„ Cross-encoderï¼Œä½¿ç”¨é»˜èªåˆ†æ•¸"""

    async def rank(self, query: str, passages: list[str]) -> list[tuple[str, float]]:
        """ç°¡å–®çš„æ’åºå¯¦ç¾"""
        # ç‚ºæ‰€æœ‰æ®µè½è¿”å›ç›¸åŒçš„åˆ†æ•¸ï¼Œä¿æŒåŸå§‹é †åº
        return [(passage, 1.0) for passage in passages]


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""

    print("=" * 70)
    print("ğŸš€ å„ªåŒ–çš„ Ollama + Graphiti è§£æ±ºæ–¹æ¡ˆ")
    print("=" * 70)

    # ç’°å¢ƒæª¢æŸ¥
    print("\nğŸ“‹ ç’°å¢ƒé…ç½®:")
    print(f"  Neo4j URI: {os.getenv('NEO4J_URI', 'bolt://localhost:7687')}")
    print(f"  Neo4j User: {os.getenv('NEO4J_USER', 'neo4j')}")
    print(f"  Model: qwen2.5:14b")
    print(f"  Embedder: nomic-embed-text:v1.5")

    try:
        # åˆå§‹åŒ– LLM
        print("\nğŸ¤– åˆå§‹åŒ– Ollama LLM...")
        llm_config = LLMConfig(
            api_key="not-needed",
            model="qwen2.5:14b",  # ä½¿ç”¨æ›´å¼·å¤§çš„æ¨¡å‹
            base_url="http://localhost:11434",
            temperature=0.1  # ç¨å¾®æé«˜æº«åº¦ä»¥ç²å¾—æ›´å¥½çš„å‰µé€ æ€§
        )
        llm_client = OptimizedOllamaClient(llm_config)
        print("  âœ… LLM åˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ–åµŒå…¥å™¨
        print("\nğŸ§² åˆå§‹åŒ–åµŒå…¥å™¨...")
        embedder = OllamaEmbedder(
            model="nomic-embed-text:v1.5",
            base_url="http://localhost:11434"
        )
        if not await embedder.test_connection():
            print("  âŒ åµŒå…¥å™¨é€£æ¥å¤±æ•—")
            return False
        print("  âœ… åµŒå…¥å™¨åˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ– Cross-encoder
        print("\nğŸ”„ åˆå§‹åŒ– Cross-encoder...")
        cross_encoder = SimpleCrossEncoder()
        print("  âœ… Cross-encoder åˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ– Graphiti
        print("\nğŸ—ƒï¸ åˆå§‹åŒ– Graphiti...")
        graphiti = Graphiti(
            uri=os.getenv('NEO4J_URI', 'bolt://localhost:7687'),
            user=os.getenv('NEO4J_USER', 'neo4j'),
            password=os.getenv('NEO4J_PASSWORD', '24927108'),
            llm_client=llm_client,
            embedder=embedder,
            cross_encoder=cross_encoder
        )

        await graphiti.build_indices_and_constraints()
        print("  âœ… Graphiti åˆå§‹åŒ–æˆåŠŸ")

        # å®šç¾© entity typesï¼ˆå­—å…¸æ ¼å¼ï¼‰
        entity_types = {
            "person": {"name": "person", "description": "People, users, developers"},
            "organization": {"name": "organization", "description": "Companies, organizations, teams"},
            "technology": {"name": "technology", "description": "Technologies, frameworks, tools, languages"},
            "concept": {"name": "concept", "description": "Concepts, methods, patterns, practices"},
            "place": {"name": "place", "description": "Locations, places"},
            "other": {"name": "other", "description": "Other entities"}
        }

        # æ·»åŠ æ¸¬è©¦è¨˜æ†¶
        print("\nğŸ“ æ·»åŠ æ¸¬è©¦è¨˜æ†¶...")
        test_episodes = [
            {
                "name": "ç”¨æˆ¶åå¥½",
                "content": "ç”¨æˆ¶ RD-CAT åå¥½ä½¿ç”¨ TypeScript é€²è¡Œé–‹ç™¼ã€‚TypeScript æä¾›äº†é¡å‹å®‰å…¨ã€‚"
            },
            {
                "name": "æŠ€è¡“çŸ¥è­˜",
                "content": "React 18 å¼•å…¥äº† Concurrent Featuresã€‚é€™åŒ…æ‹¬ Suspense å’Œ useTransitionã€‚"
            },
            {
                "name": "æœ€ä½³å¯¦è¸",
                "content": "API éŒ¯èª¤è™•ç†æ‡‰è©²åŒ…å«éŒ¯èª¤æ—¥èªŒã€å‹å–„è¨Šæ¯å’Œé‡è©¦æ©Ÿåˆ¶ã€‚"
            }
        ]

        episode_results = []
        for episode in test_episodes:
            print(f"\n  ğŸ“Œ {episode['name']}")
            try:
                result = await graphiti.add_episode(
                    name=episode['name'],
                    episode_body=episode['content'],
                    source_description="Ollama æ¸¬è©¦",
                    reference_time=datetime.now(timezone.utc)
                    # ä¸å‚³å…¥ entity_typesï¼Œä½¿ç”¨é è¨­å€¼
                )
                episode_results.append(result)

                if hasattr(result, 'episode_id'):
                    print(f"    âœ“ Episode: {result.episode_id[:8]}...")
                if hasattr(result, 'node_ids') and result.node_ids:
                    print(f"    âœ“ ç¯€é»: {len(result.node_ids)} å€‹")
                if hasattr(result, 'edge_ids') and result.edge_ids:
                    print(f"    âœ“ é—œä¿‚: {len(result.edge_ids)} å€‹")

            except Exception as e:
                print(f"    âŒ éŒ¯èª¤: {str(e)}")
                import traceback
                traceback.print_exc()

        # ç­‰å¾…è™•ç†
        print("\nâ³ ç­‰å¾…è™•ç†ï¼ˆ5ç§’ï¼‰...")
        await asyncio.sleep(5)

        # æ¸¬è©¦æœç´¢
        print("\nğŸ” æ¸¬è©¦æœç´¢åŠŸèƒ½:")
        queries = ["TypeScript", "React", "éŒ¯èª¤è™•ç†", "RD-CAT"]

        for query in queries:
            print(f"\n  ğŸ” '{query}'")
            try:
                results = await graphiti.search(query=query, num_results=3)
                if results:
                    print(f"    âœ… æ‰¾åˆ° {len(results)} å€‹çµæœ")
                else:
                    print(f"    âš ï¸ ç„¡çµæœ")
            except Exception as e:
                print(f"    âŒ éŒ¯èª¤: {str(e)[:30]}...")

        # ç¸½çµ
        print("\n" + "=" * 70)
        print("ğŸ“Š ç¸½çµ")
        print("=" * 70)

        total_episodes = len(episode_results)
        total_nodes = sum(
            len(r.node_ids) if hasattr(r, 'node_ids') and r.node_ids else 0
            for r in episode_results
        )

        print(f"\nâœ… æˆåŠŸæ·»åŠ  {total_episodes} å€‹è¨˜æ†¶")
        print(f"âœ… æå–äº† {total_nodes} å€‹å¯¦é«”")

        if total_nodes == 0:
            print("\nâš ï¸ å¯¦é«”æå–å¯èƒ½éœ€è¦:")
            print("  1. æ›´å¼·å¤§çš„æ¨¡å‹ï¼ˆå¦‚ qwen2.5:14bï¼‰")
            print("  2. æˆ–ä¸‹è¼‰å°ˆé–€çš„æ¨¡å‹ï¼ˆå¦‚ deepseek-r1:7bï¼‰")
            print("  3. èª¿æ•´æç¤ºè©ä»¥é©æ‡‰ç•¶å‰æ¨¡å‹")

        print("\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        if 'graphiti' in locals():
            await graphiti.close()
            print("\nâœ… é€£æ¥å·²é—œé–‰")

    return True


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)