#!/usr/bin/env python3
"""
Graphiti Ollama MCP Server
è§£æ±ºç´¢å¼•éŒ¯èª¤å’Œæå‡ç©©å®šæ€§
"""

import argparse
import asyncio
import logging
import os
import sys
import time
import uuid
from datetime import datetime, timezone
from typing import Any, List, Optional

from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP
from pydantic import BaseModel, Field

# å°å…¥æ–°çš„é…ç½®å’ŒéŒ¯èª¤è™•ç†ç³»çµ±
from src.config import GraphitiConfig, load_config
from src.exceptions import (
    GraphitiMCPError, OllamaError, Neo4jError, EmbeddingError,
    handle_exception, create_error_response, CommonErrors
)
from src.logging_setup import (
    setup_logging, log_system_info, log_config_summary,
    log_operation_start, log_operation_success, log_operation_error,
    performance_logger
)

# ä½¿ç”¨æˆ‘å€‘çš„è‡ªå®šç¾© Ollama çµ„ä»¶
from src.ollama_graphiti_client import OptimizedOllamaClient
from src.ollama_embedder import OllamaEmbedder
from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.edges import EntityEdge
from graphiti_core.nodes import EpisodicNode, EpisodeType
from graphiti_core.search.search_config_recipes import (
    NODE_HYBRID_SEARCH_NODE_DISTANCE,
    NODE_HYBRID_SEARCH_RRF,
)
from graphiti_core.search.search_filters import SearchFilters

load_dotenv()

# å…¨å±€é…ç½®å’Œæ—¥èªŒ
app_config: GraphitiConfig = None
logger = None
graphiti_instance = None  # å¿«å– Graphiti å¯¦ä¾‹

# éšŠåˆ—ç³»çµ± (å¾å®˜æ–¹ç‰ˆæœ¬ç§»æ¤)
episode_queues: dict[str, asyncio.Queue] = {}
queue_workers: dict[str, bool] = {}

# MCP å·¥å…·ä¸å†éœ€è¦ Pydantic æ¨¡å‹åƒæ•¸ï¼Œå·²æ”¹ç‚ºæ¨™æº–å‡½æ•¸åƒæ•¸

async def process_episode_queue(group_id: str):
    """è™•ç†ç‰¹å®š group_id çš„è¨˜æ†¶ç‰‡æ®µéšŠåˆ—"""
    global queue_workers

    logging.info(f'Starting episode queue worker for group_id: {group_id}')
    queue_workers[group_id] = True

    try:
        while True:
            # å¾éšŠåˆ—ç²å–ä¸‹ä¸€å€‹è™•ç†å‡½æ•¸
            process_func = await episode_queues[group_id].get()

            try:
                # è™•ç†è¨˜æ†¶ç‰‡æ®µ
                await process_func()
            except Exception as e:
                logging.error(f'Error processing queued episode for group_id {group_id}: {str(e)}')
            finally:
                # æ¨™è¨˜ä»»å‹™å®Œæˆ
                episode_queues[group_id].task_done()
    except asyncio.CancelledError:
        logging.info(f'Episode queue worker for group_id {group_id} was cancelled')
    except Exception as e:
        logging.error(f'Unexpected error in queue worker for group_id {group_id}: {str(e)}')
    finally:
        queue_workers[group_id] = False
        logging.info(f'Stopped episode queue worker for group_id: {group_id}')


async def initialize_graphiti():
    """åˆå§‹åŒ– Graphiti å¯¦ä¾‹ï¼ˆä½¿ç”¨å¿«å–æ©Ÿåˆ¶ï¼‰"""
    global graphiti_instance

    if graphiti_instance is not None:
        return graphiti_instance

    start_time = time.time()
    log_operation_start("initialize_graphiti")

    try:
        # å˜—è©¦å‰µå»º Ollama LLM å®¢æˆ¶ç«¯ï¼Œå¤±æ•—æ™‚è¨­ç‚º None
        llm_client = None
        try:
            llm_config = LLMConfig(
                base_url=app_config.ollama.base_url,
                model=app_config.ollama.model,
                temperature=app_config.ollama.temperature
            )
            llm_client = OptimizedOllamaClient(config=llm_config)
            print("âœ… LLM å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ LLM å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨ None: {e}")
            llm_client = None

        # å‰µå»º Ollama åµŒå…¥å™¨
        embedder = OllamaEmbedder(
            model=app_config.embedder.model,
            base_url=app_config.embedder.base_url,
            dimensions=app_config.embedder.dimensions
        )

        # å‰µå»º Graphiti å¯¦ä¾‹
        graphiti_instance = Graphiti(
            uri=app_config.neo4j.uri,
            user=app_config.neo4j.user,
            password=app_config.neo4j.password,
            llm_client=llm_client,
            embedder=embedder,
            max_coroutines=3  # é™åˆ¶ä½µç™¼æ•¸é‡é¿å… IndexError
        )

        duration = time.time() - start_time
        log_operation_success("initialize_graphiti", duration)

        return graphiti_instance

    except Exception as e:
        duration = time.time() - start_time
        log_operation_error("initialize_graphiti", e, duration=duration)

        # è½‰æ›ç‚ºçµæ§‹åŒ–ç•°å¸¸
        if isinstance(e, GraphitiMCPError):
            raise e
        else:
            raise handle_exception(e, "Graphiti åˆå§‹åŒ–å¤±æ•—")

# å‰µå»º FastMCP æ‡‰ç”¨
mcp = FastMCP("graphiti-ollama-memory")

@mcp.tool()
async def add_memory_simple(
    name: str,
    episode_body: str,
    group_id: str = "default",
    source_description: str = "MCP Server"
) -> dict:
    """ä½¿ç”¨å®‰å…¨æ–¹æ³•æ·»åŠ è¨˜æ†¶ - å®Œå…¨é¿é–‹ IndexError å•é¡Œ
    
    Args:
        name: è¨˜æ†¶ç‰‡æ®µçš„åç¨±
        episode_body: è¨˜æ†¶ç‰‡æ®µçš„å…§å®¹
        group_id: è¨˜æ†¶åˆ†çµ„ ID (é è¨­: default)
        source_description: ä¾†æºæè¿° (é è¨­: MCP Server)
    """
    start_time = time.time()
    log_operation_start("add_memory_safe", name=name[:50])

    try:
        from src.safe_memory_add import safe_add_memory

        graphiti = await initialize_graphiti()

        # ä½¿ç”¨å®‰å…¨æ–¹æ³•æ·»åŠ è¨˜æ†¶ - ç›´æ¥å‰µå»º EpisodicNode
        result = await safe_add_memory(
            graphiti,
            name=name,
            content=episode_body,
            group_id=group_id,
            source_description=source_description
        )

        duration = time.time() - start_time

        if result["success"]:
            log_operation_success("add_memory_safe", duration, name=name)
            return {
                "success": True,
                "message": result["message"],
                "uuid": result["uuid"],
                "group_id": group_id,
                "processing_time": f"{duration:.2f}s",
                "method": "safe_direct_node_creation",
                "note": "ä½¿ç”¨å®‰å…¨æ–¹æ³•ï¼Œå®Œå…¨é¿é–‹å¯¦é«”æå–ä»¥é˜²æ­¢ IndexError"
            }
        else:
            log_operation_error("add_memory_safe", Exception(result["error"]), duration=duration)
            return create_error_response(
                CommonErrors.operation_failed("add_memory_safe", result["error"]),
                f"å®‰å…¨è¨˜æ†¶æ·»åŠ å¤±æ•—: {result['error']}")

    except Exception as e:
        duration = time.time() - start_time
        log_operation_error("add_memory_safe", e, duration=duration)
        return create_error_response(
            CommonErrors.operation_failed("add_memory_safe", str(e)),
            f"è¨˜æ†¶æ·»åŠ éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

@mcp.tool()
async def search_memory_nodes(
    query: str,
    max_nodes: int = 10,
    group_ids: Optional[List[str]] = None
) -> dict:
    """æœç´¢è¨˜æ†¶ç¯€é»
    
    Args:
        query: æœå°‹é—œéµå­—
        max_nodes: è¿”å›ç¯€é»çš„æœ€å¤§æ•¸é‡ (é è¨­: 10)
        group_ids: ç”¨æ–¼ç¯©é¸çš„åˆ†çµ„ ID (é è¨­: None)
    """
    start_time = time.time()
    log_operation_start("search_nodes", query=query[:50])

    try:
        graphiti = await initialize_graphiti()

        # å‰µå»ºæœç´¢éæ¿¾å™¨
        search_filters = SearchFilters(
            group_ids=group_ids or []
        )

        # é…ç½®æœç´¢
        search_config = NODE_HYBRID_SEARCH_RRF.model_copy(deep=True)
        search_config.limit = min(max_nodes, 50)

        # åŸ·è¡Œæœç´¢ (ä½¿ç”¨ç§æœ‰ _search æ–¹æ³•)
        search_results = await graphiti._search(
            query=query,
            config=search_config,
            group_ids=group_ids or [],
            search_filter=search_filters
        )

        # å¾æœç´¢çµæœä¸­ç²å–ç¯€é»
        nodes = search_results.nodes if search_results.nodes else []

        duration = time.time() - start_time
        log_operation_success("search_nodes", duration, result_count=len(nodes))

        # ç°¡åŒ–ç¯€é»è³‡è¨Š
        simplified_nodes = []
        for node in nodes:
            simplified_nodes.append({
                "name": getattr(node, 'name', ''),
                "uuid": str(getattr(node, 'uuid', '')),
                "created_at": str(getattr(node, 'created_at', '')),
                "summary": getattr(node, 'summary', '')[:200],  # é™åˆ¶æ‘˜è¦é•·åº¦
                "group_id": getattr(node, 'group_id', ''),
                "labels": getattr(node, 'labels', [])
            })

        return {
            "message": f"æ‰¾åˆ° {len(nodes)} å€‹ç›¸é—œç¯€é»",
            "nodes": simplified_nodes,
            "duration": round(duration, 2)
        }

    except Exception as e:
        duration = time.time() - start_time
        log_operation_error("search_nodes", e, query=query[:50], duration=duration)
        return create_error_response(e, "æœç´¢ç¯€é»å¤±æ•—")

@mcp.tool()
async def search_memory_facts(
    query: str,
    max_facts: int = 10,
    group_ids: Optional[List[str]] = None
) -> dict:
    """æœç´¢è¨˜æ†¶äº‹å¯¦
    
    Args:
        query: æœå°‹é—œéµå­—
        max_facts: è¿”å›äº‹å¯¦çš„æœ€å¤§æ•¸é‡ (é è¨­: 10)
        group_ids: ç”¨æ–¼ç¯©é¸çš„åˆ†çµ„ ID (é è¨­: None)
    """
    start_time = time.time()
    log_operation_start("search_facts", query=query[:50])

    try:
        graphiti = await initialize_graphiti()

        # å‰µå»ºæœç´¢éæ¿¾å™¨
        search_filters = SearchFilters(
            group_ids=group_ids or []
        )

        # åŸ·è¡Œæœç´¢
        edges = await graphiti.search(
            query=query,
            num_results=min(max_facts, 50),  # é™åˆ¶æœ€å¤§çµæœæ•¸
            search_filter=search_filters
        )

        duration = time.time() - start_time
        log_operation_success("search_facts", duration, result_count=len(edges))

        # ç°¡åŒ–é‚Šè³‡è¨Š
        simplified_edges = []
        for edge in edges:
            simplified_edges.append({
                "relation_type": getattr(edge, 'relation_type', ''),
                "uuid": str(getattr(edge, 'uuid', '')),
                "created_at": str(getattr(edge, 'created_at', '')),
                "fact": getattr(edge, 'fact', '')[:200],  # é™åˆ¶äº‹å¯¦é•·åº¦
                "group_id": getattr(edge, 'group_id', ''),
                "source_node_uuid": str(getattr(edge, 'source_node_uuid', '')),
                "target_node_uuid": str(getattr(edge, 'target_node_uuid', ''))
            })

        return {
            "message": f"æ‰¾åˆ° {len(edges)} å€‹ç›¸é—œäº‹å¯¦",
            "facts": simplified_edges,
            "duration": round(duration, 2)
        }

    except Exception as e:
        duration = time.time() - start_time
        log_operation_error("search_facts", e, query=query[:50], duration=duration)
        return create_error_response(e, "æœç´¢äº‹å¯¦å¤±æ•—")

@mcp.tool()
async def get_episodes(
    last_n: int = 10,
    group_id: str = ""
) -> dict:
    """ç²å–æœ€è¿‘çš„è¨˜æ†¶ç‰‡æ®µ
    
    Args:
        last_n: ç²å–æœ€è¿‘è¨˜æ†¶ç‰‡æ®µçš„æ•¸é‡ (é è¨­: 10)
        group_id: ç”¨æ–¼ç¯©é¸çš„åˆ†çµ„ ID (é è¨­: "")
    """
    start_time = time.time()
    log_operation_start("get_episodes", last_n=last_n)

    try:
        graphiti = await initialize_graphiti()

        # ç²å–æœ€è¿‘çš„è¨˜æ†¶ç‰‡æ®µ
        episodes = await graphiti.retrieve_episodes(
            reference_time=datetime.now(timezone.utc),
            group_ids=[group_id] if group_id else None,
            last_n=min(last_n, 50)  # é™åˆ¶æœ€å¤§æ•¸é‡
        )

        duration = time.time() - start_time
        log_operation_success("get_episodes", duration, result_count=len(episodes))

        # ç°¡åŒ–è¨˜æ†¶ç‰‡æ®µè³‡è¨Š
        simplified_episodes = []
        for episode in episodes:
            simplified_episodes.append({
                "name": getattr(episode, 'name', ''),
                "content": getattr(episode, 'content', '')[:500],  # é™åˆ¶å…§å®¹é•·åº¦
                "uuid": str(getattr(episode, 'uuid', '')),
                "group_id": getattr(episode, 'group_id', ''),
                "created_at": str(getattr(episode, 'created_at', ''))
            })

        return {
            "message": f"æ‰¾åˆ° {len(episodes)} å€‹è¨˜æ†¶ç‰‡æ®µ",
            "episodes": simplified_episodes,
            "duration": round(duration, 2)
        }

    except Exception as e:
        duration = time.time() - start_time
        log_operation_error("get_episodes", e, last_n=last_n, duration=duration)
        return create_error_response(e, "ç²å–è¨˜æ†¶ç‰‡æ®µå¤±æ•—")

@mcp.tool()
async def test_connection() -> dict:
    """æ¸¬è©¦é€£æ¥ç‹€æ…‹"""
    try:
        start_time = time.time()

        # æ¸¬è©¦ Neo4j é€£æ¥
        graphiti = await initialize_graphiti()

        # æ¸¬è©¦ Ollama LLM
        llm_status = "OK"
        try:
            # ä½¿ç”¨æ­£ç¢ºçš„è¨Šæ¯åˆ—è¡¨æ ¼å¼
            test_response = await graphiti.llm_client.generate_response([
                {"role": "user", "content": "è«‹å›ç­”ï¼š1+1=?"}
            ])
            if not test_response:
                llm_status = "å›æ‡‰ç‚ºç©º"
        except Exception as e:
            llm_status = f"éŒ¯èª¤: {str(e)[:100]}"

        # æ¸¬è©¦åµŒå…¥å™¨
        embedder_status = "æ­£å¸¸"
        try:
            test_embedding = await graphiti.embedder.create([{"text": "æ¸¬è©¦"}])
            if not test_embedding or len(test_embedding) == 0:
                embedder_status = "åµŒå…¥ç”Ÿæˆå¤±æ•—"
        except Exception as e:
            embedder_status = f"éŒ¯èª¤: {str(e)[:100]}"

        duration = time.time() - start_time

        return {
            "message": "é€£æ¥æ¸¬è©¦å®Œæˆ",
            "neo4j": "OK",
            "ollama_llm": llm_status,
            "embedder": embedder_status,
            "duration": round(duration, 2)
        }

    except Exception as e:
        return create_error_response(e, "é€£æ¥æ¸¬è©¦å¤±æ•—")

@mcp.tool()
async def clear_graph() -> dict:
    """æ¸…é™¤åœ–è³‡æ–™åº«"""
    try:
        start_time = time.time()
        graphiti = await initialize_graphiti()

        # æ¸…é™¤åœ–è³‡æ–™åº«
        await graphiti.clear()

        # é‡ç½®å¿«å–çš„å¯¦ä¾‹
        global graphiti_instance
        graphiti_instance = None

        duration = time.time() - start_time

        return {
            "message": "åœ–è³‡æ–™åº«å·²æ¸…é™¤",
            "duration": round(duration, 2)
        }

    except Exception as e:
        return create_error_response(e, "æ¸…é™¤åœ–è³‡æ–™åº«å¤±æ•—")

def main():
    """ä¸»ç¨‹åºå…¥å£é»"""
    global app_config, logger

    parser = argparse.ArgumentParser(description="Graphiti Ollama MCP Server")
    parser.add_argument("--transport", default="stdio", choices=["stdio", "sse"])
    parser.add_argument("--config", help="é…ç½®æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--host", default="localhost", help="SSE æ¨¡å¼ä¸»æ©Ÿåœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="SSE æ¨¡å¼ç«¯å£")
    parser.add_argument("--group-id", help="åœ–å½¢å‘½åç©ºé–“ IDï¼Œç”¨æ–¼çµ„ç¹”ç›¸é—œæ•¸æ“š")

    args = parser.parse_args()

    try:
        # è¼‰å…¥é…ç½®
        app_config = load_config(args.config)

        # è¨­ç½®æ—¥èªŒ
        logger = setup_logging(app_config.logging)

        # è¨˜éŒ„ç³»çµ±ä¿¡æ¯
        log_system_info()

        # è½‰æ›é…ç½®ç‚ºå­—å…¸æ ¼å¼
        config_dict = {
            "ollama_model": app_config.ollama.model,
            "neo4j_uri": app_config.neo4j.uri,
            "embedder_model": app_config.embedder.model,
            "log_level": app_config.logging.level
        }
        log_config_summary(config_dict)

        import logging
        main_logger = logging.getLogger("main")
        main_logger.info("âœ… Graphiti + Ollama MCP æœå‹™å™¨åˆå§‹åŒ–å®Œæˆ")

        # æ ¹æ“šå‚³è¼¸æ–¹å¼é‹è¡Œ
        if args.transport == "stdio":
            import asyncio
            asyncio.run(mcp.run_stdio_async())
        elif args.transport == "sse":
            import asyncio
            asyncio.run(mcp.run_sse_async())

    except KeyboardInterrupt:
        import logging
        main_logger = logging.getLogger("main")
        main_logger.info("ğŸ‘‹ æœå‹™å™¨å·²åœæ­¢")
        sys.exit(0)
    except Exception as e:
        import logging
        error_logger = logging.getLogger("main")
        error_logger.error(f"âŒ æœå‹™å™¨å•Ÿå‹•å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()