#!/usr/bin/env python3
"""
Graphiti Ollama MCP Server - æ•´åˆ Ollama æœ¬åœ° LLM çš„ MCP æœå‹™å™¨
åŸºæ–¼æˆ‘å€‘çš„ Graphiti + Ollama è§£æ±ºæ–¹æ¡ˆ

æ•´åˆ Ollama æœ¬åœ° LLM çš„çŸ¥è­˜åœ–è­œè¨˜æ†¶ç®¡ç†æœå‹™
"""

import argparse
import asyncio
import os
import sys
import time
from datetime import datetime, timezone
from typing import Any, List, Optional

from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP
from pydantic import BaseModel, Field

# å°å…¥æ–°çš„é…ç½®å’ŒéŒ¯èª¤è™•ç†ç³»çµ±
from src.config import GraphitiConfig, load_config
from src.exceptions import (
    GraphitiMCPError, OllamaError, Neo4jError, EmbeddingError,
    handle_exception, create_error_response, CommonErrors
)
from src.logging_setup import (
    setup_logging, log_system_info, log_config_summary,
    log_operation_start, log_operation_success, log_operation_error,
    performance_logger
)

# ä½¿ç”¨æˆ‘å€‘çš„è‡ªå®šç¾© Ollama çµ„ä»¶
from src.ollama_graphiti_client import OptimizedOllamaClient
from src.ollama_embedder import OllamaEmbedder
from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.edges import EntityEdge
from graphiti_core.nodes import EpisodicNode
from graphiti_core.search.search_config_recipes import (
    NODE_HYBRID_SEARCH_NODE_DISTANCE,
    NODE_HYBRID_SEARCH_RRF,
)
from graphiti_core.search.search_filters import SearchFilters

load_dotenv()

# å…¨å±€é…ç½®å’Œæ—¥èªŒ
app_config: GraphitiConfig = None
logger = None

# MCP å·¥å…·çš„åƒæ•¸æ¨¡å‹
class AddMemoryArgs(BaseModel):
    name: str = Field(description="è¨˜æ†¶ç‰‡æ®µçš„åç¨±")
    episode_body: str = Field(description="è¨˜æ†¶ç‰‡æ®µçš„å…§å®¹")
    source_description: str = Field(default="MCP Server", description="ä¾†æºæè¿°")
    group_id: str = Field(default="default", description="è¨˜æ†¶åˆ†çµ„ ID")

class SearchNodesArgs(BaseModel):
    query: str = Field(description="æœå°‹é—œéµå­—")
    max_nodes: int = Field(default=10, description="è¿”å›ç¯€é»çš„æœ€å¤§æ•¸é‡")
    group_ids: List[str] = Field(default_factory=list, description="ç”¨æ–¼ç¯©é¸çš„åˆ†çµ„ ID")

class SearchFactsArgs(BaseModel):
    query: str = Field(description="æœå°‹é—œéµå­—")
    max_facts: int = Field(default=10, description="è¿”å›äº‹å¯¦çš„æœ€å¤§æ•¸é‡")
    group_ids: List[str] = Field(default_factory=list, description="ç”¨æ–¼ç¯©é¸çš„åˆ†çµ„ ID")

class GetEpisodesArgs(BaseModel):
    last_n: int = Field(default=10, description="ç²å–æœ€è¿‘è¨˜æ†¶ç‰‡æ®µçš„æ•¸é‡")
    group_id: str = Field(default="", description="ç”¨æ–¼ç¯©é¸çš„åˆ†çµ„ ID")

# å…¨å±€è®Šé‡
graphiti_instance: Graphiti = None


def initialize_system(config_path: Optional[str] = None):
    """åˆå§‹åŒ–ç³»çµ±é…ç½®å’Œæ—¥èªŒ"""
    global app_config, logger

    # è¼‰å…¥é…ç½®
    app_config = load_config(config_path)

    # è¨­ç½®æ—¥èªŒç³»çµ±
    log_manager = setup_logging(app_config.logging)
    logger = log_manager.get_logger("main")

    # è¨˜éŒ„ç³»çµ±å•Ÿå‹•ä¿¡æ¯
    log_system_info()
    log_config_summary(app_config.get_summary())

    # é©—è­‰é…ç½®
    if not app_config.validate():
        logger.warning("é…ç½®é©—è­‰å¤±æ•—ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ç„¡æ³•æ­£å¸¸é‹ä½œ")
    else:
        logger.info("âœ… ç³»çµ±é…ç½®é©—è­‰é€šé")

    return app_config

async def initialize_graphiti():
    """åˆå§‹åŒ– Graphiti å¯¦ä¾‹ä½¿ç”¨é…ç½®ç³»çµ±"""
    global graphiti_instance, app_config, logger

    if graphiti_instance is not None:
        return graphiti_instance

    start_time = time.time()
    log_operation_start("initialize_graphiti")

    try:
        # åˆå§‹åŒ– LLM å®¢æˆ¶ç«¯
        logger.info(f"ğŸ“¡ ä½¿ç”¨ LLM æ¨¡å‹: {app_config.ollama.model}")

        llm_config = LLMConfig(
            api_key="not-needed",
            model=app_config.ollama.model,
            base_url=app_config.ollama.base_url,
            temperature=app_config.ollama.temperature,
            max_tokens=app_config.ollama.max_tokens
        )
        llm_client = OptimizedOllamaClient(llm_config)

        # åˆå§‹åŒ–åµŒå…¥å™¨
        logger.info(f"ğŸ§² ä½¿ç”¨åµŒå…¥æ¨¡å‹: {app_config.embedder.model}")

        embedder = OllamaEmbedder(
            model=app_config.embedder.model,
            base_url=app_config.embedder.base_url,
            dimensions=app_config.embedder.dimensions
        )

        # æ¸¬è©¦é€£æ¥
        connected = await embedder.test_connection()
        if not connected:
            raise CommonErrors.ollama_connection_failed(app_config.embedder.base_url)

        # åˆå§‹åŒ– Graphiti (æ³¨æ„ï¼šGraphiti ä¸æ”¯æ´ database åƒæ•¸)
        logger.info("åˆå§‹åŒ– Graphiti æ ¸å¿ƒ...")
        graphiti_instance = Graphiti(
            uri=app_config.neo4j.uri,
            user=app_config.neo4j.user,
            password=app_config.neo4j.password,
            llm_client=llm_client,
            embedder=embedder
        )

        # å»ºç«‹ç´¢å¼•
        logger.info("å»ºç«‹ Neo4j ç´¢å¼•å’Œç´„æŸ...")
        await graphiti_instance.build_indices_and_constraints()

        duration = time.time() - start_time
        log_operation_success("initialize_graphiti", duration)
        logger.info("âœ… Graphiti + Ollama MCP æœå‹™å™¨åˆå§‹åŒ–å®Œæˆ")

        return graphiti_instance

    except Exception as e:
        duration = time.time() - start_time
        log_operation_error("initialize_graphiti", e, duration=duration)

        # è½‰æ›ç‚ºçµæ§‹åŒ–ç•°å¸¸
        if isinstance(e, GraphitiMCPError):
            raise e
        else:
            raise handle_exception(e, "Graphiti åˆå§‹åŒ–å¤±æ•—")

# å‰µå»º FastMCP æ‡‰ç”¨
mcp = FastMCP("graphiti-ollama-memory")

@mcp.tool()
async def add_memory_simple(args: AddMemoryArgs) -> dict:
    """æ·»åŠ è¨˜æ†¶åˆ° Graphitiï¼ˆä½¿ç”¨æ–°çš„éŒ¯èª¤è™•ç†ç³»çµ±ï¼‰"""
    start_time = time.time()
    log_operation_start("add_memory", name=args.name, group_id=args.group_id)

    try:
        graphiti = await initialize_graphiti()

        result = await graphiti.add_episode(
            name=args.name,
            episode_body=args.episode_body,
            source_description=args.source_description,
            group_id=args.group_id,
            reference_time=datetime.now(timezone.utc)
        )

        duration = time.time() - start_time

        # ç°¡åŒ–çš„æˆåŠŸè¨˜éŒ„ï¼Œä¸æå–è¤‡é›œçµ±è¨ˆä¿¡æ¯
        log_operation_success(
            "add_memory",
            duration,
            episode_id=getattr(result, 'episode_id', None)
        )

        # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
        performance_logger.log_memory_add_performance(
            len(args.episode_body), duration, True
        )

        return {
            "success": True,
            "message": f"è¨˜æ†¶ '{args.name}' æ–°å¢æˆåŠŸ",
            "episode_id": getattr(result, 'episode_id', None),
            "duration": round(duration, 2)
        }

    except Exception as e:
        duration = time.time() - start_time
        log_operation_error("add_memory", e, name=args.name, duration=duration)

        # è¨˜éŒ„å¤±æ•—çš„æ€§èƒ½æŒ‡æ¨™
        performance_logger.log_memory_add_performance(
            len(args.episode_body), duration, False
        )

        return create_error_response(e, "æ–°å¢è¨˜æ†¶å¤±æ•—")

@mcp.tool()
async def search_memory_nodes(args: SearchNodesArgs) -> dict:
    """æœç´¢è¨˜æ†¶ç¯€é»"""
    try:
        graphiti = await initialize_graphiti()

        # ä½¿ç”¨ Graphiti çš„ _search API é€²è¡Œèªæ„æœç´¢
        effective_group_ids = args.group_ids if args.group_ids else []

        # é…ç½®æœç´¢åƒæ•¸
        search_config = NODE_HYBRID_SEARCH_RRF.model_copy(deep=True)
        search_config.limit = args.max_nodes

        filters = SearchFilters()

        # ä½¿ç”¨ Graphiti çš„ _search API
        search_results = await graphiti._search(
            query=args.query,
            config=search_config,
            group_ids=effective_group_ids,
            center_node_uuid=None,
            search_filter=filters,
        )

        nodes = []
        if search_results and hasattr(search_results, 'nodes') and search_results.nodes:
            try:
                for node in search_results.nodes:
                    try:
                        # å®‰å…¨åœ°æå–ç¯€é»è³‡è¨Š
                        node_data = {
                            "name": getattr(node, 'name', 'æœªçŸ¥åç¨±'),
                            "uuid": str(getattr(node, 'uuid', '')),
                            "created_at": str(getattr(node, 'created_at', '')) if hasattr(node, 'created_at') else "",
                            "summary": getattr(node, 'summary', '') or '',
                            "group_id": getattr(node, 'group_id', ''),
                            "labels": getattr(node, 'labels', []) if hasattr(node, 'labels') else []
                        }

                        # ç¢ºä¿ labels æ˜¯åˆ—è¡¨
                        if not isinstance(node_data["labels"], list):
                            node_data["labels"] = []

                        nodes.append(node_data)
                    except (AttributeError, TypeError, IndexError) as node_error:
                        logger.warning(f"è™•ç†ç¯€é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {node_error}")
                        continue
            except (TypeError, AttributeError) as nodes_error:
                logger.warning(f"è™•ç†ç¯€é»åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {nodes_error}")
                nodes = []

        return {
            "message": f"æ‰¾åˆ° {len(nodes)} å€‹ç›¸é—œç¯€é»" if nodes else "æœªæ‰¾åˆ°ç›¸é—œç¯€é»",
            "nodes": nodes
        }

    except Exception as e:
        return {
            "message": f"æœå°‹ç¯€é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
            "nodes": [],
            "error": True
        }

@mcp.tool()
async def search_memory_facts(args: SearchFactsArgs) -> dict:
    """æœç´¢è¨˜æ†¶äº‹å¯¦"""
    try:
        graphiti = await initialize_graphiti()

        # ä½¿ç”¨ Graphiti çš„ search API é€²è¡Œèªæ„æœç´¢
        effective_group_ids = args.group_ids if args.group_ids else []

        # ä½¿ç”¨ Graphiti çš„æœç´¢ API
        relevant_edges = await graphiti.search(
            group_ids=effective_group_ids,
            query=args.query,
            num_results=args.max_facts
        )

        facts = []
        if relevant_edges:
            try:
                for edge in relevant_edges:
                    try:
                        # å®‰å…¨åœ°æª¢æŸ¥ edge æ˜¯å¦æœ‰ fact å±¬æ€§
                        if hasattr(edge, 'fact') and getattr(edge, 'fact', None):
                            fact_data = {
                                "fact": getattr(edge, 'fact', ''),
                                "uuid": str(getattr(edge, 'uuid', '')),
                                "created_at": str(getattr(edge, 'created_at', '')) if hasattr(edge, 'created_at') else "",
                                "relation_type": type(edge).__name__ if edge else "unknown"
                            }

                            # å®‰å…¨åœ°ç²å–ä¾†æºå’Œç›®æ¨™å¯¦é«”åç¨±
                            try:
                                if hasattr(edge, 'source_node_uuid') and hasattr(edge, 'target_node_uuid'):
                                    fact_data["source_entity"] = str(getattr(edge, 'source_node_uuid', ''))
                                    fact_data["target_entity"] = str(getattr(edge, 'target_node_uuid', ''))
                                else:
                                    fact_data["source_entity"] = "unknown"
                                    fact_data["target_entity"] = "unknown"
                            except (AttributeError, TypeError):
                                fact_data["source_entity"] = "unknown"
                                fact_data["target_entity"] = "unknown"

                            facts.append(fact_data)
                    except (AttributeError, TypeError, IndexError) as edge_error:
                        logger.warning(f"è™•ç†äº‹å¯¦é‚Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {edge_error}")
                        continue
            except (TypeError, AttributeError) as edges_error:
                logger.warning(f"è™•ç†äº‹å¯¦åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {edges_error}")
                facts = []

        return {
            "message": f"æ‰¾åˆ° {len(facts)} å€‹ç›¸é—œäº‹å¯¦" if facts else "æœªæ‰¾åˆ°ç›¸é—œäº‹å¯¦",
            "facts": facts
        }

    except Exception as e:
        return {
            "message": f"æœå°‹äº‹å¯¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
            "facts": [],
            "error": True
        }

@mcp.tool()
async def get_episodes(args: GetEpisodesArgs) -> dict:
    """ç²å–æœ€è¿‘çš„è¨˜æ†¶"""
    try:
        graphiti = await initialize_graphiti()

        query_text = """
        MATCH (e:Episodic)
        WHERE $group_id = '' OR e.group_id = $group_id
        RETURN e.name as name,
               e.content as content,
               e.uuid as uuid,
               e.group_id as group_id,
               e.created_at as created_at
        ORDER BY e.created_at DESC
        LIMIT $limit
        """

        results = await graphiti.driver.execute_query(
            query_text,
            group_id=args.group_id,
            limit=args.last_n
        )

        episodes = []
        if results and results.records:
            for record in results.records:
                episodes.append({
                    "name": record["name"],
                    "content": record.get("content", ""),
                    "uuid": record["uuid"],
                    "group_id": record.get("group_id", ""),
                    "created_at": str(record.get("created_at", ""))
                })

        return {
            "message": f"æ‰¾åˆ° {len(episodes)} å€‹è¨˜æ†¶ç‰‡æ®µ",
            "episodes": episodes
        }

    except Exception as e:
        return {
            "message": f"ç²å–è¨˜æ†¶ç‰‡æ®µæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
            "episodes": [],
            "error": True
        }

@mcp.tool()
async def clear_graph() -> dict:
    """æ¸…é™¤åœ–è³‡æ–™åº«"""
    try:
        graphiti = await initialize_graphiti()

        # æ¸…é™¤æ‰€æœ‰æ•¸æ“š
        await graphiti.driver.execute_query("MATCH (n) DETACH DELETE n")

        # é‡å»ºç´¢å¼•
        await graphiti.build_indices_and_constraints()

        return {
            "message": "åœ–è³‡æ–™åº«å·²æ¸…é™¤ä¸¦é‡å»ºç´¢å¼•"
        }

    except Exception as e:
        return {
            "message": f"æ¸…é™¤åœ–è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
            "error": True
        }

@mcp.tool()
async def test_connection() -> dict:
    """æ¸¬è©¦é€£æ¥ç‹€æ…‹"""
    try:
        graphiti = await initialize_graphiti()

        # æ¸¬è©¦ Neo4j é€£æ¥
        neo4j_result = await graphiti.driver.execute_query("RETURN 'OK' as status")
        neo4j_status = "OK" if neo4j_result else "FAILED"

        # æ¸¬è©¦ Ollama é€£æ¥
        llm_status = "OK"  # å¦‚æœèƒ½åˆå§‹åŒ–å°±è¡¨ç¤ºé€£æ¥æ­£å¸¸

        return {
            "message": "é€£æ¥æ¸¬è©¦å®Œæˆ",
            "neo4j": neo4j_status,
            "ollama_llm": llm_status,
            "embedder": "æ­£å¸¸"
        }

    except Exception as e:
        return {
            "message": f"é€£æ¥æ¸¬è©¦å¤±æ•—: {str(e)}",
            "error": True
        }

async def main():
    """å•Ÿå‹• MCP æœå‹™å™¨ï¼ˆä½¿ç”¨æ–°é…ç½®ç³»çµ±ï¼‰"""
    parser = argparse.ArgumentParser(description="Graphiti Ollama MCP Server")
    parser.add_argument("--transport", choices=["stdio", "sse"], help="Transport protocol")
    parser.add_argument("--host", help="Server host")
    parser.add_argument("--port", type=int, help="Server port")
    parser.add_argument("--config", help="Configuration file path")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–ç³»çµ±é…ç½®
        # èª¿æ•´é…ç½®æª”æ¡ˆè·¯å¾‘
        config_path = args.config
        if config_path and not config_path.startswith('/') and not config_path.startswith('configs/'):
            config_path = f"configs/{config_path}"

        config = initialize_system(config_path)

        # ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸è¦†è“‹é…ç½®
        if args.transport:
            config.server.transport = args.transport
        if args.host:
            config.server.host = args.host
        if args.port:
            config.server.port = args.port

        logger.info(f"ğŸš€ å•Ÿå‹• Graphiti MCP Server")
        logger.info(f"   å‚³è¼¸å”è­°: {config.server.transport}")
        logger.info(f"   æœå‹™åœ°å€: {config.server.host}:{config.server.port}")

        # è¨­ç½® FastMCP é…ç½®
        if hasattr(mcp, 'settings'):
            if config.server.host != 'localhost':
                mcp.settings.host = config.server.host
            if config.server.port != 3001:
                mcp.settings.port = config.server.port

        # å•Ÿå‹•æœå‹™å™¨
        if config.server.transport == "stdio":
            logger.info("å•Ÿå‹• STDIO æ¨¡å¼...")
            await mcp.run_stdio_async()
        elif config.server.transport == "sse":
            logger.info("å•Ÿå‹• SSE æ¨¡å¼...")
            await mcp.run_sse_async()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å‚³è¼¸å”è­°: {config.server.transport}")

    except Exception as e:
        if logger:
            logger.error(f"æœå‹™å™¨å•Ÿå‹•å¤±æ•—: {e}")
        else:
            print(f"âŒ æœå‹™å™¨å•Ÿå‹•å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())